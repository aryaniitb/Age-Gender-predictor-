{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FFNcY3WUEVE",
        "outputId": "64f91011-1eee-498e-e0a6-f7264043778f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "epyJ__2TUn6V"
      },
      "outputs": [],
      "source": [
        "FACE_DATASET_PATH = \"/content/drive/MyDrive/crop_part1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxWbfgtTUt04",
        "outputId": "9067672a-8db8-4f23-a3d5-79b26583d0e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.167-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.11/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.11.0.86)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.15.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.7.9)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.58.5)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics-8.3.167-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ultralytics-thop, ultralytics\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 ultralytics-8.3.167 ultralytics-thop-2.0.14\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision ultralytics opencv-python-headless numpy pillow requests tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RomPCJw3UxxV"
      },
      "outputs": [],
      "source": [
        "!wget -q \\\n",
        "  https://huggingface.co/ultralytics/yolov8n-face/resolve/main/yolov8n-face.pt \\\n",
        "  -O yolov8n-face.pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8VPLAcCU1pV",
        "outputId": "c0a2502e-c865-42a6-e120-9b6192fd8553"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1qcr9DbgsX3ryrz2uU8w4Xm3cOrRywXqb\n",
            "To: /content/yolov8n-face.pt\n",
            "\r  0% 0.00/6.39M [00:00<?, ?B/s]\r100% 6.39M/6.39M [00:00<00:00, 204MB/s]\n"
          ]
        }
      ],
      "source": [
        "# install gdown if you don't already have it\n",
        "!pip install --quiet gdown\n",
        "\n",
        "# download the YOLOv8-n face model\n",
        "!gdown \"https://drive.google.com/uc?id=1qcr9DbgsX3ryrz2uU8w4Xm3cOrRywXqb\" -O /content/yolov8n-face.pt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "aQKRFToeU7Ew",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0847c542-e3cd-45c1-8ee4-d1fefee6cc4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file âœ… \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.23M/6.23M [00:00<00:00, 112MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ” Loading dataset...\n",
            "Loaded 5000 face images\n",
            "âœ… Loaded dataset with 5000 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97.8M/97.8M [00:00<00:00, 189MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 4500 face images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 500 face images\n",
            "ğŸš€ Training on 4500 samples, validating on 500 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [04:30<00:00,  3.82s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/20\n",
            "Train - Age Loss: 0.0311 | Gender Loss: 0.6478 | Gender Acc: 0.6124\n",
            "Val   - Age Loss: 0.0188 | Gender Loss: 0.5487 | Gender Acc: 0.7260\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.79      0.54      0.64       226\n",
            "      Female       0.70      0.88      0.78       274\n",
            "\n",
            "    accuracy                           0.73       500\n",
            "   macro avg       0.74      0.71      0.71       500\n",
            "weighted avg       0.74      0.73      0.72       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[121 105]\n",
            " [ 32 242]]\n",
            "ğŸ’¾ Saved best model to face_age_gender_model.pth (Gender Acc: 0.7260)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:50<00:00,  1.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2/20\n",
            "Train - Age Loss: 0.0199 | Gender Loss: 0.4722 | Gender Acc: 0.7689\n",
            "Val   - Age Loss: 0.0111 | Gender Loss: 0.3583 | Gender Acc: 0.8160\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.91      0.66      0.76       226\n",
            "      Female       0.77      0.95      0.85       274\n",
            "\n",
            "    accuracy                           0.82       500\n",
            "   macro avg       0.84      0.80      0.81       500\n",
            "weighted avg       0.83      0.82      0.81       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[149  77]\n",
            " [ 15 259]]\n",
            "ğŸ’¾ Saved best model to face_age_gender_model.pth (Gender Acc: 0.8160)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:50<00:00,  1.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3/20\n",
            "Train - Age Loss: 0.0153 | Gender Loss: 0.3741 | Gender Acc: 0.8202\n",
            "Val   - Age Loss: 0.0099 | Gender Loss: 0.3104 | Gender Acc: 0.8580\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.82      0.87      0.85       226\n",
            "      Female       0.89      0.85      0.87       274\n",
            "\n",
            "    accuracy                           0.86       500\n",
            "   macro avg       0.86      0.86      0.86       500\n",
            "weighted avg       0.86      0.86      0.86       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[197  29]\n",
            " [ 42 232]]\n",
            "ğŸ’¾ Saved best model to face_age_gender_model.pth (Gender Acc: 0.8580)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:50<00:00,  1.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 4/20\n",
            "Train - Age Loss: 0.0133 | Gender Loss: 0.3208 | Gender Acc: 0.8533\n",
            "Val   - Age Loss: 0.0079 | Gender Loss: 0.2564 | Gender Acc: 0.8840\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.83      0.93      0.88       226\n",
            "      Female       0.94      0.85      0.89       274\n",
            "\n",
            "    accuracy                           0.88       500\n",
            "   macro avg       0.88      0.89      0.88       500\n",
            "weighted avg       0.89      0.88      0.88       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[210  16]\n",
            " [ 42 232]]\n",
            "ğŸ’¾ Saved best model to face_age_gender_model.pth (Gender Acc: 0.8840)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:49<00:00,  1.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 5/20\n",
            "Train - Age Loss: 0.0118 | Gender Loss: 0.2731 | Gender Acc: 0.8771\n",
            "Val   - Age Loss: 0.0077 | Gender Loss: 0.2210 | Gender Acc: 0.8920\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.86      0.90      0.88       226\n",
            "      Female       0.92      0.88      0.90       274\n",
            "\n",
            "    accuracy                           0.89       500\n",
            "   macro avg       0.89      0.89      0.89       500\n",
            "weighted avg       0.89      0.89      0.89       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[204  22]\n",
            " [ 32 242]]\n",
            "ğŸ’¾ Saved best model to face_age_gender_model.pth (Gender Acc: 0.8920)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:50<00:00,  1.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 6/20\n",
            "Train - Age Loss: 0.0113 | Gender Loss: 0.2526 | Gender Acc: 0.8913\n",
            "Val   - Age Loss: 0.0074 | Gender Loss: 0.2231 | Gender Acc: 0.9260\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.90      0.94      0.92       226\n",
            "      Female       0.95      0.91      0.93       274\n",
            "\n",
            "    accuracy                           0.93       500\n",
            "   macro avg       0.92      0.93      0.93       500\n",
            "weighted avg       0.93      0.93      0.93       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[213  13]\n",
            " [ 24 250]]\n",
            "ğŸ’¾ Saved best model to face_age_gender_model.pth (Gender Acc: 0.9260)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:51<00:00,  1.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 7/20\n",
            "Train - Age Loss: 0.0107 | Gender Loss: 0.2204 | Gender Acc: 0.9029\n",
            "Val   - Age Loss: 0.0071 | Gender Loss: 0.1902 | Gender Acc: 0.9080\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.96      0.83      0.89       226\n",
            "      Female       0.88      0.97      0.92       274\n",
            "\n",
            "    accuracy                           0.91       500\n",
            "   macro avg       0.92      0.90      0.91       500\n",
            "weighted avg       0.91      0.91      0.91       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[188  38]\n",
            " [  8 266]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:50<00:00,  1.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 8/20\n",
            "Train - Age Loss: 0.0098 | Gender Loss: 0.2047 | Gender Acc: 0.9104\n",
            "Val   - Age Loss: 0.0072 | Gender Loss: 0.1470 | Gender Acc: 0.9460\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.93      0.95      0.94       226\n",
            "      Female       0.96      0.95      0.95       274\n",
            "\n",
            "    accuracy                           0.95       500\n",
            "   macro avg       0.95      0.95      0.95       500\n",
            "weighted avg       0.95      0.95      0.95       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[214  12]\n",
            " [ 15 259]]\n",
            "ğŸ’¾ Saved best model to face_age_gender_model.pth (Gender Acc: 0.9460)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:51<00:00,  1.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 9/20\n",
            "Train - Age Loss: 0.0097 | Gender Loss: 0.1826 | Gender Acc: 0.9229\n",
            "Val   - Age Loss: 0.0071 | Gender Loss: 0.1423 | Gender Acc: 0.9440\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.91      0.98      0.94       226\n",
            "      Female       0.98      0.92      0.95       274\n",
            "\n",
            "    accuracy                           0.94       500\n",
            "   macro avg       0.94      0.95      0.94       500\n",
            "weighted avg       0.95      0.94      0.94       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[221   5]\n",
            " [ 23 251]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:50<00:00,  1.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 10/20\n",
            "Train - Age Loss: 0.0096 | Gender Loss: 0.1746 | Gender Acc: 0.9318\n",
            "Val   - Age Loss: 0.0064 | Gender Loss: 0.1031 | Gender Acc: 0.9580\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.96      0.95      0.95       226\n",
            "      Female       0.96      0.96      0.96       274\n",
            "\n",
            "    accuracy                           0.96       500\n",
            "   macro avg       0.96      0.96      0.96       500\n",
            "weighted avg       0.96      0.96      0.96       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[215  11]\n",
            " [ 10 264]]\n",
            "ğŸ’¾ Saved best model to face_age_gender_model.pth (Gender Acc: 0.9580)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:50<00:00,  1.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 11/20\n",
            "Train - Age Loss: 0.0088 | Gender Loss: 0.1574 | Gender Acc: 0.9378\n",
            "Val   - Age Loss: 0.0063 | Gender Loss: 0.0893 | Gender Acc: 0.9700\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.95      0.99      0.97       226\n",
            "      Female       0.99      0.96      0.97       274\n",
            "\n",
            "    accuracy                           0.97       500\n",
            "   macro avg       0.97      0.97      0.97       500\n",
            "weighted avg       0.97      0.97      0.97       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[223   3]\n",
            " [ 12 262]]\n",
            "ğŸ’¾ Saved best model to face_age_gender_model.pth (Gender Acc: 0.9700)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:50<00:00,  1.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 12/20\n",
            "Train - Age Loss: 0.0091 | Gender Loss: 0.1417 | Gender Acc: 0.9413\n",
            "Val   - Age Loss: 0.0069 | Gender Loss: 0.0940 | Gender Acc: 0.9560\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.95      0.96      0.95       226\n",
            "      Female       0.96      0.96      0.96       274\n",
            "\n",
            "    accuracy                           0.96       500\n",
            "   macro avg       0.96      0.96      0.96       500\n",
            "weighted avg       0.96      0.96      0.96       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[216  10]\n",
            " [ 12 262]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:50<00:00,  1.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 13/20\n",
            "Train - Age Loss: 0.0086 | Gender Loss: 0.1141 | Gender Acc: 0.9551\n",
            "Val   - Age Loss: 0.0063 | Gender Loss: 0.0845 | Gender Acc: 0.9660\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.94      0.99      0.96       226\n",
            "      Female       0.99      0.95      0.97       274\n",
            "\n",
            "    accuracy                           0.97       500\n",
            "   macro avg       0.96      0.97      0.97       500\n",
            "weighted avg       0.97      0.97      0.97       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[223   3]\n",
            " [ 14 260]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:52<00:00,  1.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 14/20\n",
            "Train - Age Loss: 0.0085 | Gender Loss: 0.1248 | Gender Acc: 0.9511\n",
            "Val   - Age Loss: 0.0060 | Gender Loss: 0.1114 | Gender Acc: 0.9560\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.92      0.99      0.95       226\n",
            "      Female       0.99      0.93      0.96       274\n",
            "\n",
            "    accuracy                           0.96       500\n",
            "   macro avg       0.95      0.96      0.96       500\n",
            "weighted avg       0.96      0.96      0.96       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[223   3]\n",
            " [ 19 255]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:51<00:00,  1.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 15/20\n",
            "Train - Age Loss: 0.0084 | Gender Loss: 0.1120 | Gender Acc: 0.9564\n",
            "Val   - Age Loss: 0.0054 | Gender Loss: 0.0535 | Gender Acc: 0.9880\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.98      1.00      0.99       226\n",
            "      Female       1.00      0.98      0.99       274\n",
            "\n",
            "    accuracy                           0.99       500\n",
            "   macro avg       0.99      0.99      0.99       500\n",
            "weighted avg       0.99      0.99      0.99       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[225   1]\n",
            " [  5 269]]\n",
            "ğŸ’¾ Saved best model to face_age_gender_model.pth (Gender Acc: 0.9880)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:50<00:00,  1.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 16/20\n",
            "Train - Age Loss: 0.0080 | Gender Loss: 0.0975 | Gender Acc: 0.9591\n",
            "Val   - Age Loss: 0.0052 | Gender Loss: 0.0476 | Gender Acc: 0.9780\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.98      0.97      0.98       226\n",
            "      Female       0.98      0.98      0.98       274\n",
            "\n",
            "    accuracy                           0.98       500\n",
            "   macro avg       0.98      0.98      0.98       500\n",
            "weighted avg       0.98      0.98      0.98       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[220   6]\n",
            " [  5 269]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:48<00:00,  1.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 17/20\n",
            "Train - Age Loss: 0.0076 | Gender Loss: 0.0978 | Gender Acc: 0.9633\n",
            "Val   - Age Loss: 0.0053 | Gender Loss: 0.0458 | Gender Acc: 0.9780\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.97      0.98      0.98       226\n",
            "      Female       0.98      0.98      0.98       274\n",
            "\n",
            "    accuracy                           0.98       500\n",
            "   macro avg       0.98      0.98      0.98       500\n",
            "weighted avg       0.98      0.98      0.98       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[221   5]\n",
            " [  6 268]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:48<00:00,  1.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 18/20\n",
            "Train - Age Loss: 0.0076 | Gender Loss: 0.0848 | Gender Acc: 0.9680\n",
            "Val   - Age Loss: 0.0059 | Gender Loss: 0.0369 | Gender Acc: 0.9820\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.97      0.99      0.98       226\n",
            "      Female       0.99      0.97      0.98       274\n",
            "\n",
            "    accuracy                           0.98       500\n",
            "   macro avg       0.98      0.98      0.98       500\n",
            "weighted avg       0.98      0.98      0.98       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[224   2]\n",
            " [  7 267]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:50<00:00,  1.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 19/20\n",
            "Train - Age Loss: 0.0077 | Gender Loss: 0.0807 | Gender Acc: 0.9689\n",
            "Val   - Age Loss: 0.0052 | Gender Loss: 0.0347 | Gender Acc: 0.9860\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.98      0.99      0.98       226\n",
            "      Female       0.99      0.98      0.99       274\n",
            "\n",
            "    accuracy                           0.99       500\n",
            "   macro avg       0.99      0.99      0.99       500\n",
            "weighted avg       0.99      0.99      0.99       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[224   2]\n",
            " [  5 269]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:48<00:00,  1.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 20/20\n",
            "Train - Age Loss: 0.0074 | Gender Loss: 0.0769 | Gender Acc: 0.9707\n",
            "Val   - Age Loss: 0.0054 | Gender Loss: 0.0390 | Gender Acc: 0.9820\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.99      0.97      0.98       226\n",
            "      Female       0.97      0.99      0.98       274\n",
            "\n",
            "    accuracy                           0.98       500\n",
            "   macro avg       0.98      0.98      0.98       500\n",
            "weighted avg       0.98      0.98      0.98       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[219   7]\n",
            " [  2 272]]\n",
            "âœ… Loaded pretrained age/gender model\n",
            "\n",
            "ğŸ” Processing Two girls.jpg...\n",
            "\n",
            "0: 480x640 2 persons, 43.0ms\n",
            "Speed: 9.6ms preprocess, 43.0ms inference, 327.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 faces, 16.7ms\n",
            "Speed: 1.9ms preprocess, 16.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "âŒ No valid detections found\n",
            "\n",
            "ğŸ” Processing download.jpg...\n",
            "\n",
            "0: 448x640 5 persons, 39.8ms\n",
            "Speed: 2.3ms preprocess, 39.8ms inference, 1.8ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 23 faces, 16.8ms\n",
            "Speed: 2.2ms preprocess, 16.8ms inference, 1.8ms postprocess per image at shape (1, 3, 448, 640)\n",
            "âŒ No valid detections found\n",
            "\n",
            "ğŸ” Processing how-to-be-a-people-person-1662995088.jpg...\n",
            "\n",
            "0: 352x640 5 persons, 57.2ms\n",
            "Speed: 6.0ms preprocess, 57.2ms inference, 2.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 5 faces, 23.3ms\n",
            "Speed: 5.7ms preprocess, 23.3ms inference, 1.7ms postprocess per image at shape (1, 3, 352, 640)\n",
            "âœ… Found 5 detections in 0.21s\n",
            "1. female (Age: 36.2, Conf: 0.68)\n",
            "2. female (Age: 27.7, Conf: 0.75)\n",
            "3. female (Age: 28.7, Conf: 0.73)\n",
            "4. female (Age: 38.7, Conf: 0.82)\n",
            "5. female (Age: 36.9, Conf: 0.69)\n",
            "ğŸ’¾ Saved visualization to output_how-to-be-a-people-person-1662995088.jpg\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torchvision import transforms, models\n",
        "from ultralytics import YOLO\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import glob\n",
        "import re\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import shutil\n",
        "from sklearn.metrics import confusion_matrix, classification_report # Import metrics\n",
        "\n",
        "# Configuration\n",
        "TRAIN_MODEL = True\n",
        "RUN_INFERENCE = True\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Paths\n",
        "FACE_DATASET_PATH = \"/content/drive/MyDrive/crop_part1\"\n",
        "MODEL_PATH = \"face_age_gender_model.pth\"\n",
        "TEST_IMAGES = [\n",
        "    \"/content/Two girls.jpg\",\n",
        "    \"/content/download.jpg\",\n",
        "    \"/content/how-to-be-a-people-person-1662995088.jpg\"\n",
        "]\n",
        "MAX_AGE = 100.0\n",
        "\n",
        "# Download YOLO models if needed\n",
        "if not os.path.exists(\"yolov8n.pt\"):\n",
        "    torch.hub.download_url_to_file(\"https://github.com/ultralytics/assets/releases/download/v8.1.0/yolov8n.pt\", \"yolov8n.pt\")\n",
        "if not os.path.exists(\"yolov8n-face.pt\"):\n",
        "    torch.hub.download_url_to_file(\"https://akanametov.github.io/yolov8-face/yolov8n-face.pt\", \"yolov8n-face.pt\")\n",
        "\n",
        "# â”€â”€â”€ Dataset â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "class FaceAgeGenderDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None, max_samples=None):\n",
        "        self.transform = transform\n",
        "        paths = glob.glob(os.path.join(root_dir, \"**\", \"*.jpg\"), recursive=True)\n",
        "        paths += glob.glob(os.path.join(root_dir, \"**\", \"*.jpeg\"), recursive=True)\n",
        "        paths += glob.glob(os.path.join(root_dir, \"**\", \"*.png\"), recursive=True)\n",
        "        self.samples = []\n",
        "\n",
        "        for p in paths:\n",
        "            fn = os.path.basename(p)\n",
        "            m = re.match(r\"(\\d+)_([01])_.*\\.(?:jpg|jpeg|png)\", fn)\n",
        "            if m:\n",
        "                try:\n",
        "                    age = min(int(m.group(1)), MAX_AGE)\n",
        "                    gender = int(m.group(2))\n",
        "                    self.samples.append((p, age, gender))\n",
        "                    if max_samples and len(self.samples) >= max_samples:\n",
        "                        break\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "        if not self.samples:\n",
        "            raise RuntimeError(\"No valid samples found\")\n",
        "        print(f\"Loaded {len(self.samples)} face images\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        p, age, gender = self.samples[idx]\n",
        "        try:\n",
        "            img = Image.open(p).convert(\"RGB\")\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "            # Normalize age between 0-1\n",
        "            norm_age = age / MAX_AGE\n",
        "            return img, norm_age, gender\n",
        "        except:\n",
        "            # Fallback for corrupted images\n",
        "            img = Image.new(\"RGB\", (224, 224), (0, 0, 0))\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "            return img, 0.0, 0\n",
        "\n",
        "# â”€â”€â”€ Enhanced Model with Attention â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "class AgeGenderModel(nn.Module):\n",
        "    def __init__(self, backbone=\"resnet50\"):\n",
        "        super().__init__()\n",
        "        if backbone == \"resnet34\":\n",
        "            base = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)\n",
        "            feat_dim = 512\n",
        "        else:\n",
        "            base = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
        "            feat_dim = 2048\n",
        "\n",
        "        # Feature extraction\n",
        "        self.features = nn.Sequential(*list(base.children())[:-1])\n",
        "\n",
        "        # Attention mechanism\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Conv2d(feat_dim, feat_dim//8, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(feat_dim//8, 1, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # Age estimation (regression)\n",
        "        self.age_head = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(feat_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid()  # Output between 0-1\n",
        "        )\n",
        "\n",
        "        # Gender classification\n",
        "        self.gender_head = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(feat_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "\n",
        "        # Initialize weights\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.features(x)\n",
        "\n",
        "        # Apply attention\n",
        "        att = self.attention(features)\n",
        "        weighted_features = features * att\n",
        "\n",
        "        # Global pooling\n",
        "        pooled = F.adaptive_avg_pool2d(weighted_features, (1, 1))\n",
        "\n",
        "        # Heads\n",
        "        age = self.age_head(pooled)\n",
        "        gender = self.gender_head(pooled)\n",
        "        return age, gender\n",
        "\n",
        "# â”€â”€â”€ Robust Face Attribute Detector â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "class FaceAttributeDetector:\n",
        "    def __init__(self, model_path=None, backbone=\"resnet50\"):\n",
        "        # Person detector (for full body)\n",
        "        self.person_detector = YOLO(\"yolov8n.pt\")\n",
        "        # Face detector (for precise face detection)\n",
        "        self.face_detector = YOLO(\"yolov8n-face.pt\")\n",
        "\n",
        "        # Age/gender model\n",
        "        self.net = AgeGenderModel(backbone).to(DEVICE)\n",
        "        self.backbone = backbone\n",
        "\n",
        "        # Transformations\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "        # Load model if available\n",
        "        if model_path and os.path.exists(model_path):\n",
        "            try:\n",
        "                self.net.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
        "                print(\"âœ… Loaded pretrained age/gender model\")\n",
        "            except Exception as e:\n",
        "                print(f\"âš ï¸ Failed to load model weights: {e}\")\n",
        "\n",
        "    def train(self, dataset, epochs=20, batch_size=64, save_path=MODEL_PATH):\n",
        "        train_size = int(0.9 * len(dataset))\n",
        "        val_size = len(dataset) - train_size\n",
        "        # Create separate datasets with different transforms\n",
        "        train_set = FaceAgeGenderDataset(\n",
        "            FACE_DATASET_PATH,\n",
        "            transform=transforms.Compose([\n",
        "                transforms.Resize(256),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.RandomRotation(10),\n",
        "                transforms.RandomAffine(0, shear=10),\n",
        "                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "                transforms.RandomCrop(224),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "                transforms.RandomErasing(p=0.3, scale=(0.02, 0.2))\n",
        "            ]),\n",
        "            max_samples=train_size\n",
        "        )\n",
        "        val_set = FaceAgeGenderDataset(\n",
        "            FACE_DATASET_PATH,\n",
        "            transform=transforms.Compose([\n",
        "                transforms.Resize(256),\n",
        "                transforms.CenterCrop(224),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "            ]),\n",
        "            max_samples=val_size\n",
        "        )\n",
        "\n",
        "\n",
        "        train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=min(4, os.cpu_count()))\n",
        "        val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=min(2, os.cpu_count()))\n",
        "\n",
        "        age_loss_fn = nn.MSELoss()\n",
        "        gender_loss_fn = nn.BCEWithLogitsLoss()\n",
        "        optimizer = optim.AdamW(self.net.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
        "\n",
        "        print(f\"ğŸš€ Training on {len(train_set)} samples, validating on {len(val_set)} samples\")\n",
        "        best_val_gender_acc = 0.0\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            self.net.train()\n",
        "            train_age_loss, train_gender_loss = 0.0, 0.0\n",
        "            correct_gender, total_samples = 0, 0\n",
        "\n",
        "            for imgs, ages, genders in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
        "                imgs = imgs.to(DEVICE)\n",
        "                ages = ages.float().to(DEVICE)\n",
        "                genders = genders.float().to(DEVICE).unsqueeze(1)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                age_preds, gender_preds = self.net(imgs)\n",
        "\n",
        "                age_loss = age_loss_fn(age_preds.squeeze(), ages)\n",
        "                gender_loss = gender_loss_fn(gender_preds, genders)\n",
        "                loss = 0.5 * age_loss + 0.5 * gender_loss\n",
        "\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.net.parameters(), 1.0)\n",
        "                optimizer.step()\n",
        "\n",
        "                train_age_loss += age_loss.item()\n",
        "                train_gender_loss += gender_loss.item()\n",
        "\n",
        "                gender_preds_bin = (torch.sigmoid(gender_preds) > 0.5).float()\n",
        "                correct_gender += (gender_preds_bin == genders).sum().item()\n",
        "                total_samples += imgs.size(0)\n",
        "\n",
        "            self.net.eval()\n",
        "            val_age_loss, val_gender_loss = 0.0, 0.0\n",
        "            val_correct_gender, val_total = 0, 0\n",
        "            y_true, y_pred = [], []\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for imgs, ages, genders in val_loader:\n",
        "                    imgs = imgs.to(DEVICE)\n",
        "                    ages = ages.float().to(DEVICE)\n",
        "                    genders = genders.float().to(DEVICE).unsqueeze(1)\n",
        "\n",
        "                    age_preds, gender_preds = self.net(imgs)\n",
        "\n",
        "                    val_age_loss += age_loss_fn(age_preds.squeeze(), ages).item()\n",
        "                    val_gender_loss += gender_loss_fn(gender_preds, genders).item()\n",
        "\n",
        "                    gender_preds_bin = (torch.sigmoid(gender_preds) > 0.5).float()\n",
        "                    val_correct_gender += (gender_preds_bin == genders).sum().item()\n",
        "                    val_total += imgs.size(0)\n",
        "\n",
        "                    y_true.extend(genders.cpu().numpy().flatten())\n",
        "                    y_pred.extend(gender_preds_bin.cpu().numpy().flatten())\n",
        "\n",
        "            train_age_loss /= len(train_loader)\n",
        "            train_gender_loss /= len(train_loader)\n",
        "            val_age_loss /= len(val_loader)\n",
        "            val_gender_loss /= len(val_loader)\n",
        "            train_gender_acc = correct_gender / total_samples\n",
        "            val_gender_acc = val_correct_gender / val_total\n",
        "            val_combined_loss = 0.5 * val_age_loss + 0.5 * val_gender_loss\n",
        "\n",
        "            print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "            print(f\"Train - Age Loss: {train_age_loss:.4f} | Gender Loss: {train_gender_loss:.4f} | Gender Acc: {train_gender_acc:.4f}\")\n",
        "            print(f\"Val   - Age Loss: {val_age_loss:.4f} | Gender Loss: {val_gender_loss:.4f} | Gender Acc: {val_gender_acc:.4f}\")\n",
        "            print(\"Classification Report:\")\n",
        "            print(classification_report(y_true, y_pred, target_names=[\"Male\", \"Female\"]))\n",
        "            print(\"Confusion Matrix:\")\n",
        "            print(confusion_matrix(y_true, y_pred))\n",
        "\n",
        "            # Save model based on better gender accuracy\n",
        "            if val_gender_acc > best_val_gender_acc:\n",
        "                best_val_gender_acc = val_gender_acc\n",
        "                torch.save(self.net.state_dict(), save_path)\n",
        "                print(f\"ğŸ’¾ Saved best model to {save_path} (Gender Acc: {val_gender_acc:.4f})\")\n",
        "\n",
        "            scheduler.step(val_combined_loss)\n",
        "\n",
        "\n",
        "    def predict(self, image_path, min_face_size=40, min_confidence=0.5):\n",
        "        \"\"\"Predict age and gender for all faces in an image\"\"\"\n",
        "        # Load image\n",
        "        img = cv2.imread(image_path)\n",
        "        if img is None:\n",
        "            print(f\"âŒ Could not read image: {image_path}\")\n",
        "            return []\n",
        "\n",
        "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        detections = []\n",
        "\n",
        "        # Detect persons\n",
        "        person_results = self.person_detector(img_rgb, conf=0.5, classes=[0])\n",
        "        person_boxes = []\n",
        "        if hasattr(person_results[0], 'boxes') and person_results[0].boxes is not None:\n",
        "            for box in person_results[0].boxes:\n",
        "                if int(box.cls) == 0:  # Person class\n",
        "                    x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())\n",
        "                    person_boxes.append((x1, y1, x2, y2))\n",
        "\n",
        "        # Detect faces\n",
        "        face_results = self.face_detector(img_rgb, conf=0.3)\n",
        "        face_boxes = []\n",
        "        if hasattr(face_results[0], 'boxes') and face_results[0].boxes is not None:\n",
        "            for box in face_results[0].boxes:\n",
        "                if int(box.cls) == 0:  # Face class\n",
        "                    x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())\n",
        "                    conf = box.conf.item()\n",
        "                    # Skip small faces\n",
        "                    if (x2 - x1) < min_face_size or (y2 - y1) < min_face_size:\n",
        "                        continue\n",
        "                    face_boxes.append((x1, y1, x2, y2, conf))\n",
        "\n",
        "        # Process each detected person\n",
        "        for pbox in person_boxes:\n",
        "            px1, py1, px2, py2 = pbox\n",
        "            # Find faces within this person\n",
        "            person_faces = []\n",
        "            for fbox in face_boxes:\n",
        "                fx1, fy1, fx2, fy2, conf = fbox\n",
        "                # Check if face center is within person box\n",
        "                cx = (fx1 + fx2) // 2\n",
        "                cy = (fy1 + fy2) // 2\n",
        "                if (px1 <= cx <= px2) and (py1 <= cy <= py2):\n",
        "                    person_faces.append((fx1, fy1, fx2, fy2, conf))\n",
        "\n",
        "            # Process faces for this person\n",
        "            for face in person_faces:\n",
        "                fx1, fy1, fx2, fy2, conf = face\n",
        "                face_img = img_rgb[fy1:fy2, fx1:fx2]\n",
        "                if face_img.size == 0:\n",
        "                    continue\n",
        "\n",
        "                # Predict attributes\n",
        "                with torch.no_grad():\n",
        "                    face_pil = Image.fromarray(face_img)\n",
        "                    t = self.transform(face_pil).unsqueeze(0).to(DEVICE)\n",
        "                    age_pred, gender_pred = self.net(t)\n",
        "\n",
        "                    # Process predictions\n",
        "                    age = age_pred[0][0].item() * MAX_AGE\n",
        "                    gender_prob = torch.sigmoid(gender_pred[0][0]).item()\n",
        "                    gender = \"female\" if gender_prob > 0.5 else \"male\"\n",
        "                    gender_conf = gender_prob if gender == \"female\" else 1 - gender_prob\n",
        "\n",
        "                    # Combined confidence\n",
        "                    face_conf = min(conf, (conf + gender_conf) / 2)\n",
        "\n",
        "                if face_conf >= min_confidence:\n",
        "                    detections.append({\n",
        "                        \"person_bbox\": (px1, py1, px2, py2),\n",
        "                        \"face_bbox\": (fx1, fy1, fx2, fy2),\n",
        "                        \"age\": age,\n",
        "                        \"gender\": gender,\n",
        "                        \"conf\": face_conf,\n",
        "                        \"gender_conf\": gender_conf\n",
        "                    })\n",
        "\n",
        "        # Process standalone faces not in any person\n",
        "        for fbox in face_boxes:\n",
        "            fx1, fy1, fx2, fy2, conf = fbox\n",
        "            # Check if already processed\n",
        "            processed = any(fx1 == d[\"face_bbox\"][0] for d in detections)\n",
        "            if processed:\n",
        "                continue\n",
        "\n",
        "            face_img = img_rgb[fy1:fy2, fx1:fx2]\n",
        "            if face_img.size == 0:\n",
        "                continue\n",
        "\n",
        "            # Create approximate person box\n",
        "            w, h = fx2 - fx1, fy2 - fy1\n",
        "            px1 = max(0, fx1 - w//2)\n",
        "            py1 = max(0, fy1 - h//2)\n",
        "            px2 = min(img.shape[1], fx2 + w//2)\n",
        "            py2 = min(img.shape[0], fy2 + h*2)\n",
        "\n",
        "            # Predict attributes\n",
        "            with torch.no_grad():\n",
        "                face_pil = Image.fromarray(face_img)\n",
        "                t = self.transform(face_pil).unsqueeze(0).to(DEVICE)\n",
        "                age_pred, gender_pred = self.net(t)\n",
        "\n",
        "                # Process predictions\n",
        "                age = age_pred[0][0].item() * MAX_AGE\n",
        "                gender_prob = torch.sigmoid(gender_pred[0][0]).item()\n",
        "                gender = \"female\" if gender_prob > 0.5 else \"male\"\n",
        "                gender_conf = gender_prob if gender == \"female\" else 1 - gender_prob\n",
        "\n",
        "                # Combined confidence\n",
        "                face_conf = min(conf, (conf + gender_conf) / 2)\n",
        "\n",
        "            if face_conf >= min_confidence:\n",
        "                detections.append({\n",
        "                    \"person_bbox\": (px1, py1, px2, py2),\n",
        "                    \"face_bbox\": (fx1, fy1, fx2, fy2),\n",
        "                    \"age\": age,\n",
        "                    \"gender\": gender,\n",
        "                    \"conf\": face_conf,\n",
        "                    \"gender_conf\": gender_conf\n",
        "                })\n",
        "\n",
        "        return detections\n",
        "\n",
        "# â”€â”€â”€ Visualization â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def visualize_results(image_path, detections, min_confidence=0.5):\n",
        "    \"\"\"Draw detection results on the image\"\"\"\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        print(f\"âŒ Could not read image: {image_path}\")\n",
        "        return \"\"\n",
        "\n",
        "    # Define colors\n",
        "    person_color = (0, 255, 0)  # Green for person\n",
        "    face_color = (0, 0, 255)    # Red for face\n",
        "    text_color = (255, 255, 255) # White text\n",
        "\n",
        "    for d in detections:\n",
        "        if d[\"conf\"] < min_confidence:\n",
        "            continue\n",
        "\n",
        "        # Unpack detections\n",
        "        px1, py1, px2, py2 = d[\"person_bbox\"]\n",
        "        fx1, fy1, fx2, fy2 = d[\"face_bbox\"]\n",
        "        gender = d[\"gender\"]\n",
        "        age = d[\"age\"]\n",
        "        conf = d[\"conf\"]\n",
        "\n",
        "        # Draw boxes\n",
        "        cv2.rectangle(img, (px1, py1), (px2, py2), person_color, 2)\n",
        "        cv2.rectangle(img, (fx1, fy1), (fx2, fy2), face_color, 2)\n",
        "\n",
        "        # Label\n",
        "        label = f\"{gender} {age:.1f}y (Conf: {conf:.2f})\"\n",
        "        cv2.putText(img, label, (px1, py1 - 10),\n",
        "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, text_color, 2)\n",
        "\n",
        "    # Save output\n",
        "    out_path = f\"output_{os.path.basename(image_path)}\"\n",
        "    cv2.imwrite(out_path, img)\n",
        "    print(f\"ğŸ’¾ Saved visualization to {out_path}\")\n",
        "    return out_path\n",
        "\n",
        "# â”€â”€â”€ Main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "if __name__ == \"__main__\":\n",
        "    # Training\n",
        "    if TRAIN_MODEL:\n",
        "        print(\"ğŸ” Loading dataset...\")\n",
        "        try:\n",
        "            transform = transforms.Compose([\n",
        "                transforms.Resize(256),\n",
        "                transforms.CenterCrop(224),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "            ])\n",
        "            dataset = FaceAgeGenderDataset(FACE_DATASET_PATH, transform=transform, max_samples=5000)\n",
        "            print(f\"âœ… Loaded dataset with {len(dataset)} samples\")\n",
        "\n",
        "            # Initialize and train detector\n",
        "            detector = FaceAttributeDetector(backbone=\"resnet50\")\n",
        "            detector.train(dataset, epochs=20, batch_size=64)\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error during training: {str(e)}\")\n",
        "\n",
        "    # Inference\n",
        "    if RUN_INFERENCE:\n",
        "        # Initialize detector with trained model\n",
        "        detector = FaceAttributeDetector(model_path=MODEL_PATH, backbone=\"resnet50\")\n",
        "\n",
        "        for img_path in TEST_IMAGES:\n",
        "            if not os.path.exists(img_path):\n",
        "                print(f\"âŒ Image not found: {img_path}\")\n",
        "                continue\n",
        "\n",
        "            print(f\"\\nğŸ” Processing {os.path.basename(img_path)}...\")\n",
        "            try:\n",
        "                start_time = time.time()\n",
        "                detections = detector.predict(img_path, min_confidence=0.4)\n",
        "                proc_time = time.time() - start_time\n",
        "\n",
        "                if detections:\n",
        "                    print(f\"âœ… Found {len(detections)} detections in {proc_time:.2f}s\")\n",
        "                    for i, d in enumerate(detections, 1):\n",
        "                        print(f\"{i}. {d['gender']} (Age: {d['age']:.1f}, Conf: {d['conf']:.2f})\")\n",
        "                    visualize_results(img_path, detections)\n",
        "                else:\n",
        "                    print(\"âŒ No valid detections found\")\n",
        "            except Exception as e:\n",
        "                print(f\"âŒ Error during inference: {str(e)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torchvision import transforms, models\n",
        "from ultralytics import YOLO\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import glob\n",
        "import re\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Configuration\n",
        "TRAIN_MODEL = True\n",
        "RUN_INFERENCE = True\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Paths\n",
        "FACE_DATASET_PATH = \"/content/drive/MyDrive/crop_part1\"\n",
        "MODEL_PATH = \"face_age_gender_model.pth\"\n",
        "TEST_IMAGES = [\n",
        "    \"/content/Two girls.jpg\",\n",
        "    \"/content/download.jpg\",\n",
        "    \"/content/how-to-be-a-people-person-1662995088.jpg\"\n",
        "]\n",
        "MAX_AGE = 100.0\n",
        "\n",
        "# Download YOLO face model if needed\n",
        "if not os.path.exists(\"yolov8n-face.pt\"):\n",
        "    torch.hub.download_url_to_file(\"https://github.com/ultralytics/assets/releases/download/v8.1.0/yolov8n-face.pt\", \"yolov8n-face.pt\")\n",
        "\n",
        "# â”€â”€â”€ Dataset â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "class FaceAgeGenderDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None, max_samples=None):\n",
        "        self.transform = transform\n",
        "        paths = glob.glob(os.path.join(root_dir, \"**\", \"*.jpg\"), recursive=True)\n",
        "        paths += glob.glob(os.path.join(root_dir, \"**\", \"*.jpeg\"), recursive=True)\n",
        "        paths += glob.glob(os.path.join(root_dir, \"**\", \"*.png\"), recursive=True)\n",
        "        self.samples = []\n",
        "\n",
        "        for p in paths:\n",
        "            fn = os.path.basename(p)\n",
        "            m = re.match(r\"(\\d+)_([01])_.*\\.(?:jpg|jpeg|png)\", fn)\n",
        "            if m:\n",
        "                try:\n",
        "                    age = min(int(m.group(1)), MAX_AGE)\n",
        "                    gender = int(m.group(2))\n",
        "                    self.samples.append((p, age, gender))\n",
        "                    if max_samples and len(self.samples) >= max_samples:\n",
        "                        break\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "        if not self.samples:\n",
        "            raise RuntimeError(\"No valid samples found\")\n",
        "        print(f\"Loaded {len(self.samples)} face images\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        p, age, gender = self.samples[idx]\n",
        "        try:\n",
        "            img = Image.open(p).convert(\"RGB\")\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "            norm_age = age / MAX_AGE\n",
        "            return img, norm_age, gender\n",
        "        except:\n",
        "            img = Image.new(\"RGB\", (224, 224), (0, 0, 0))\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "            return img, 0.0, 0\n",
        "\n",
        "# â”€â”€â”€ Model with Attention â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "class AgeGenderModel(nn.Module):\n",
        "    def __init__(self, backbone=\"resnet50\"):\n",
        "        super().__init__()\n",
        "        if backbone == \"resnet34\":\n",
        "            base = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)\n",
        "            feat_dim = 512\n",
        "        else:\n",
        "            base = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
        "            feat_dim = 2048\n",
        "\n",
        "        self.features = nn.Sequential(*list(base.children())[:-1])\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Conv2d(feat_dim, feat_dim//8, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(feat_dim//8, 1, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.age_head = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(feat_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.gender_head = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(feat_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.features(x)\n",
        "        att = self.attention(features)\n",
        "        weighted_features = features * att\n",
        "        pooled = F.adaptive_avg_pool2d(weighted_features, (1, 1))\n",
        "        age = self.age_head(pooled)\n",
        "        gender = self.gender_head(pooled)\n",
        "        return age, gender\n",
        "\n",
        "# â”€â”€â”€ Face Attribute Detector â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "class FaceAttributeDetector:\n",
        "    def __init__(self, model_path=None, backbone=\"resnet50\"):\n",
        "        self.face_detector = YOLO(\"yolov8n-face.pt\")\n",
        "        self.net = AgeGenderModel(backbone).to(DEVICE)\n",
        "        self.backbone = backbone\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "        if model_path and os.path.exists(model_path):\n",
        "            try:\n",
        "                self.net.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
        "                print(\"âœ… Loaded pretrained age/gender model\")\n",
        "            except Exception as e:\n",
        "                print(f\"âš ï¸ Failed to load model weights: {e}\")\n",
        "\n",
        "    def train(self, dataset, epochs=20, batch_size=64, save_path=MODEL_PATH):\n",
        "        train_size = int(0.9 * len(dataset))\n",
        "        val_size = len(dataset) - train_size\n",
        "        train_set = FaceAgeGenderDataset(\n",
        "            FACE_DATASET_PATH,\n",
        "            transform=transforms.Compose([\n",
        "                transforms.Resize(256),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.RandomRotation(10),\n",
        "                transforms.RandomAffine(0, shear=10),\n",
        "                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "                transforms.RandomCrop(224),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "                transforms.RandomErasing(p=0.3, scale=(0.02, 0.2))\n",
        "            ]),\n",
        "            max_samples=train_size\n",
        "        )\n",
        "        val_set = FaceAgeGenderDataset(\n",
        "            FACE_DATASET_PATH,\n",
        "            transform=transforms.Compose([\n",
        "                transforms.Resize(256),\n",
        "                transforms.CenterCrop(224),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "            ]),\n",
        "            max_samples=val_size\n",
        "        )\n",
        "        train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=min(4, os.cpu_count()))\n",
        "        val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=min(2, os.cpu_count()))\n",
        "\n",
        "        age_loss_fn = nn.MSELoss()\n",
        "        gender_loss_fn = nn.BCEWithLogitsLoss()\n",
        "        optimizer = optim.AdamW(self.net.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
        "\n",
        "        print(f\"ğŸš€ Training on {len(train_set)} samples, validating on {len(val_set)} samples\")\n",
        "        best_val_gender_acc = 0.0\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            self.net.train()\n",
        "            train_age_loss, train_gender_loss = 0.0, 0.0\n",
        "            correct_gender, total_samples = 0, 0\n",
        "\n",
        "            for imgs, ages, genders in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
        "                imgs = imgs.to(DEVICE)\n",
        "                ages = ages.float().to(DEVICE)\n",
        "                genders = genders.float().to(DEVICE).unsqueeze(1)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                age_preds, gender_preds = self.net(imgs)\n",
        "\n",
        "                age_loss = age_loss_fn(age_preds.squeeze(), ages)\n",
        "                gender_loss = gender_loss_fn(gender_preds, genders)\n",
        "                loss = 0.5 * age_loss + 0.5 * gender_loss\n",
        "\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.net.parameters(), 1.0)\n",
        "                optimizer.step()\n",
        "\n",
        "                train_age_loss += age_loss.item()\n",
        "                train_gender_loss += gender_loss.item()\n",
        "                gender_preds_bin = (torch.sigmoid(gender_preds) > 0.5).float()\n",
        "                correct_gender += (gender_preds_bin == genders).sum().item()\n",
        "                total_samples += imgs.size(0)\n",
        "\n",
        "            self.net.eval()\n",
        "            val_age_loss, val_gender_loss = 0.0, 0.0\n",
        "            val_correct_gender, val_total = 0, 0\n",
        "            y_true, y_pred = [], []\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for imgs, ages, genders in val_loader:\n",
        "                    imgs = imgs.to(DEVICE)\n",
        "                    ages = ages.float().to(DEVICE)\n",
        "                    genders = genders.float().to(DEVICE).unsqueeze(1)\n",
        "\n",
        "                    age_preds, gender_preds = self.net(imgs)\n",
        "                    val_age_loss += age_loss_fn(age_preds.squeeze(), ages).item()\n",
        "                    val_gender_loss += gender_loss_fn(gender_preds, genders).item()\n",
        "                    gender_preds_bin = (torch.sigmoid(gender_preds) > 0.5).float()\n",
        "                    val_correct_gender += (gender_preds_bin == genders).sum().item()\n",
        "                    val_total += imgs.size(0)\n",
        "                    y_true.extend(genders.cpu().numpy().flatten())\n",
        "                    y_pred.extend(gender_preds_bin.cpu().numpy().flatten())\n",
        "\n",
        "            train_age_loss /= len(train_loader)\n",
        "            train_gender_loss /= len(train_loader)\n",
        "            val_age_loss /= len(val_loader)\n",
        "            val_gender_loss /= len(val_loader)\n",
        "            train_gender_acc = correct_gender / total_samples\n",
        "            val_gender_acc = val_correct_gender / val_total\n",
        "            val_combined_loss = 0.5 * val_age_loss + 0.5 * val_gender_loss\n",
        "\n",
        "            print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "            print(f\"Train - Age Loss: {train_age_loss:.4f} | Gender Loss: {train_gender_loss:.4f} | Gender Acc: {train_gender_acc:.4f}\")\n",
        "            print(f\"Val   - Age Loss: {val_age_loss:.4f} | Gender Loss: {val_gender_loss:.4f} | Gender Acc: {val_gender_acc:.4f}\")\n",
        "            print(\"Classification Report:\")\n",
        "            print(classification_report(y_true, y_pred, target_names=[\"Male\", \"Female\"]))\n",
        "            print(\"Confusion Matrix:\")\n",
        "            print(confusion_matrix(y_true, y_pred))\n",
        "\n",
        "            if val_gender_acc > best_val_gender_acc:\n",
        "                best_val_gender_acc = val_gender_acc\n",
        "                torch.save(self.net.state_dict(), save_path)\n",
        "                print(f\"ğŸ’¾ Saved best model to {save_path} (Gender Acc: {val_gender_acc:.4f})\")\n",
        "\n",
        "            scheduler.step(val_combined_loss)\n",
        "\n",
        "    def predict(self, image_path, min_face_size=30, min_confidence=0.3):\n",
        "        img = cv2.imread(image_path)\n",
        "        if img is None:\n",
        "            print(f\"âŒ Could not read image: {image_path}\")\n",
        "            return []\n",
        "\n",
        "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        detections = []\n",
        "\n",
        "        # Face detection with lower confidence threshold\n",
        "        face_results = self.face_detector(img_rgb, conf=0.2)\n",
        "        face_boxes = []\n",
        "        if hasattr(face_results[0], 'boxes') and face_results[0].boxes is not None:\n",
        "            for box in face_results[0].boxes:\n",
        "                if int(box.cls) == 0:\n",
        "                    x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())\n",
        "                    conf = box.conf.item()\n",
        "                    if (x2 - x1) < min_face_size or (y2 - y1) < min_face_size:\n",
        "                        continue\n",
        "                    face_boxes.append((x1, y1, x2, y2, conf))\n",
        "\n",
        "        for fbox in face_boxes:\n",
        "            fx1, fy1, fx2, fy2, conf = fbox\n",
        "            face_img = img_rgb[fy1:fy2, fx1:fx2]\n",
        "            if face_img.size == 0:\n",
        "                continue\n",
        "\n",
        "            with torch.no_grad():\n",
        "                face_pil = Image.fromarray(face_img)\n",
        "                t = self.transform(face_pil).unsqueeze(0).to(DEVICE)\n",
        "                age_pred, gender_pred = self.net(t)\n",
        "\n",
        "                age = age_pred[0][0].item() * MAX_AGE\n",
        "                gender_prob = torch.sigmoid(gender_pred[0][0]).item()\n",
        "                gender = \"female\" if gender_prob > 0.5 else \"male\"\n",
        "                gender_conf = gender_prob if gender == \"female\" else 1 - gender_prob\n",
        "                face_conf = (conf + gender_conf) / 2\n",
        "\n",
        "            if face_conf >= min_confidence:\n",
        "                detections.append({\n",
        "                    \"face_bbox\": (fx1, fy1, fx2, fy2),\n",
        "                    \"age\": age,\n",
        "                    \"gender\": gender,\n",
        "                    \"conf\": face_conf,\n",
        "                    \"gender_conf\": gender_conf\n",
        "                })\n",
        "\n",
        "        return detections\n",
        "\n",
        "# â”€â”€â”€ Visualization â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def visualize_results(image_path, detections, min_confidence=0.3):\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        print(f\"âŒ Could not read image: {image_path}\")\n",
        "        return \"\"\n",
        "\n",
        "    face_color = (0, 0, 255)\n",
        "    text_color = (255, 255, 255)\n",
        "\n",
        "    for d in detections:\n",
        "        if d[\"conf\"] < min_confidence:\n",
        "            continue\n",
        "        fx1, fy1, fx2, fy2 = d[\"face_bbox\"]\n",
        "        gender = d[\"gender\"]\n",
        "        age = d[\"age\"]\n",
        "        conf = d[\"conf\"]\n",
        "        cv2.rectangle(img, (fx1, fy1), (fx2, fy2), face_color, 2)\n",
        "        label = f\"{gender} {age:.1f}y (Conf: {conf:.2f})\"\n",
        "        cv2.putText(img, label, (fx1, fy1 - 10),\n",
        "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, text_color, 2)\n",
        "\n",
        "    out_path = f\"output_{os.path.basename(image_path)}\"\n",
        "    cv2.imwrite(out_path, img)\n",
        "    print(f\"ğŸ’¾ Saved visualization to {out_path}\")\n",
        "    return out_path\n",
        "\n",
        "# â”€â”€â”€ Main â”€â”€â”€â”€\n",
        "if __name__ == \"__main__\":\n",
        "    if TRAIN_MODEL:\n",
        "        print(\"ğŸ” Loading dataset...\")\n",
        "        try:\n",
        "            transform = transforms.Compose([\n",
        "                transforms.Resize(256),\n",
        "                transforms.CenterCrop(224),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "            ])\n",
        "            dataset = FaceAgeGenderDataset(FACE_DATASET_PATH, transform=transform, max_samples=5000)\n",
        "            print(f\"âœ… Loaded dataset with {len(dataset)} samples\")\n",
        "            detector = FaceAttributeDetector(backbone=\"resnet50\")\n",
        "            detector.train(dataset, epochs=20, batch_size=64)\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error during training: {str(e)}\")\n",
        "\n",
        "    if RUN_INFERENCE:\n",
        "        detector = FaceAttributeDetector(model_path=MODEL_PATH, backbone=\"resnet50\")\n",
        "        for img_path in TEST_IMAGES:\n",
        "            if not os.path.exists(img_path):\n",
        "                print(f\"âŒ Image not found: {img_path}\")\n",
        "                continue\n",
        "            print(f\"\\nğŸ” Processing {os.path.basename(img_path)}...\")\n",
        "            try:\n",
        "                start_time = time.time()\n",
        "                detections = detector.predict(img_path, min_confidence=0.3)\n",
        "                proc_time = time.time() - start_time\n",
        "                if detections:\n",
        "                    print(f\"âœ… Found {len(detections)} detections in {proc_time:.2f}s\")\n",
        "                    for i, d in enumerate(detections, 1):\n",
        "                        print(f\"{i}. {d['gender']} (Age: {d['age']:.1f}, Conf: {d['conf']:.2f})\")\n",
        "                    visualize_results(img_path, detections)\n",
        "                else:\n",
        "                    print(\"âŒ No valid detections found\")\n",
        "            except Exception as e:\n",
        "                print(f\"âŒ Error during inference: {str(e)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30dPc6LTUCda",
        "outputId": "dff66848-59d8-4c26-bd12-959c81c8c6e4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ” Loading dataset...\n",
            "Loaded 5000 face images\n",
            "âœ… Loaded dataset with 5000 samples\n",
            "Loaded 4500 face images\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 500 face images\n",
            "ğŸš€ Training on 4500 samples, validating on 500 samples\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [01:40<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/20\n",
            "Train - Age Loss: 0.0289 | Gender Loss: 0.6465 | Gender Acc: 0.6089\n",
            "Val   - Age Loss: 0.0194 | Gender Loss: 0.5477 | Gender Acc: 0.7040\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.89      0.39      0.55       226\n",
            "      Female       0.66      0.96      0.78       274\n",
            "\n",
            "    accuracy                           0.70       500\n",
            "   macro avg       0.77      0.68      0.66       500\n",
            "weighted avg       0.76      0.70      0.67       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[ 89 137]\n",
            " [ 11 263]]\n",
            "ğŸ’¾ Saved best model to face_age_gender_model.pth (Gender Acc: 0.7040)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:49<00:00,  1.43it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 2/20\n",
            "Train - Age Loss: 0.0200 | Gender Loss: 0.4765 | Gender Acc: 0.7709\n",
            "Val   - Age Loss: 0.0115 | Gender Loss: 0.3239 | Gender Acc: 0.8460\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.89      0.75      0.82       226\n",
            "      Female       0.82      0.92      0.87       274\n",
            "\n",
            "    accuracy                           0.85       500\n",
            "   macro avg       0.85      0.84      0.84       500\n",
            "weighted avg       0.85      0.85      0.84       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[170  56]\n",
            " [ 21 253]]\n",
            "ğŸ’¾ Saved best model to face_age_gender_model.pth (Gender Acc: 0.8460)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:50<00:00,  1.40it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 3/20\n",
            "Train - Age Loss: 0.0160 | Gender Loss: 0.3614 | Gender Acc: 0.8289\n",
            "Val   - Age Loss: 0.0092 | Gender Loss: 0.3168 | Gender Acc: 0.8640\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.94      0.74      0.83       226\n",
            "      Female       0.82      0.96      0.89       274\n",
            "\n",
            "    accuracy                           0.86       500\n",
            "   macro avg       0.88      0.85      0.86       500\n",
            "weighted avg       0.88      0.86      0.86       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[168  58]\n",
            " [ 10 264]]\n",
            "ğŸ’¾ Saved best model to face_age_gender_model.pth (Gender Acc: 0.8640)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:49<00:00,  1.42it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 4/20\n",
            "Train - Age Loss: 0.0137 | Gender Loss: 0.3139 | Gender Acc: 0.8549\n",
            "Val   - Age Loss: 0.0087 | Gender Loss: 0.2553 | Gender Acc: 0.8780\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.89      0.83      0.86       226\n",
            "      Female       0.87      0.92      0.89       274\n",
            "\n",
            "    accuracy                           0.88       500\n",
            "   macro avg       0.88      0.87      0.88       500\n",
            "weighted avg       0.88      0.88      0.88       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[187  39]\n",
            " [ 22 252]]\n",
            "ğŸ’¾ Saved best model to face_age_gender_model.pth (Gender Acc: 0.8780)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:48<00:00,  1.46it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 5/20\n",
            "Train - Age Loss: 0.0127 | Gender Loss: 0.2829 | Gender Acc: 0.8769\n",
            "Val   - Age Loss: 0.0089 | Gender Loss: 0.2240 | Gender Acc: 0.8980\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.89      0.88      0.89       226\n",
            "      Female       0.90      0.91      0.91       274\n",
            "\n",
            "    accuracy                           0.90       500\n",
            "   macro avg       0.90      0.90      0.90       500\n",
            "weighted avg       0.90      0.90      0.90       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[199  27]\n",
            " [ 24 250]]\n",
            "ğŸ’¾ Saved best model to face_age_gender_model.pth (Gender Acc: 0.8980)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:48<00:00,  1.47it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 6/20\n",
            "Train - Age Loss: 0.0115 | Gender Loss: 0.2483 | Gender Acc: 0.8900\n",
            "Val   - Age Loss: 0.0080 | Gender Loss: 0.1956 | Gender Acc: 0.9160\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.89      0.92      0.91       226\n",
            "      Female       0.94      0.91      0.92       274\n",
            "\n",
            "    accuracy                           0.92       500\n",
            "   macro avg       0.91      0.92      0.92       500\n",
            "weighted avg       0.92      0.92      0.92       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[209  17]\n",
            " [ 25 249]]\n",
            "ğŸ’¾ Saved best model to face_age_gender_model.pth (Gender Acc: 0.9160)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:49<00:00,  1.44it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 7/20\n",
            "Train - Age Loss: 0.0108 | Gender Loss: 0.2275 | Gender Acc: 0.9029\n",
            "Val   - Age Loss: 0.0084 | Gender Loss: 0.1877 | Gender Acc: 0.9060\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.96      0.83      0.89       226\n",
            "      Female       0.87      0.97      0.92       274\n",
            "\n",
            "    accuracy                           0.91       500\n",
            "   macro avg       0.92      0.90      0.90       500\n",
            "weighted avg       0.91      0.91      0.91       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[187  39]\n",
            " [  8 266]]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:47<00:00,  1.48it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 8/20\n",
            "Train - Age Loss: 0.0100 | Gender Loss: 0.1990 | Gender Acc: 0.9104\n",
            "Val   - Age Loss: 0.0073 | Gender Loss: 0.1555 | Gender Acc: 0.9420\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.94      0.93      0.94       226\n",
            "      Female       0.95      0.95      0.95       274\n",
            "\n",
            "    accuracy                           0.94       500\n",
            "   macro avg       0.94      0.94      0.94       500\n",
            "weighted avg       0.94      0.94      0.94       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[211  15]\n",
            " [ 14 260]]\n",
            "ğŸ’¾ Saved best model to face_age_gender_model.pth (Gender Acc: 0.9420)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:49<00:00,  1.44it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 9/20\n",
            "Train - Age Loss: 0.0096 | Gender Loss: 0.1877 | Gender Acc: 0.9220\n",
            "Val   - Age Loss: 0.0073 | Gender Loss: 0.1356 | Gender Acc: 0.9320\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.91      0.94      0.93       226\n",
            "      Female       0.95      0.93      0.94       274\n",
            "\n",
            "    accuracy                           0.93       500\n",
            "   macro avg       0.93      0.93      0.93       500\n",
            "weighted avg       0.93      0.93      0.93       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[212  14]\n",
            " [ 20 254]]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:49<00:00,  1.44it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 10/20\n",
            "Train - Age Loss: 0.0094 | Gender Loss: 0.1662 | Gender Acc: 0.9333\n",
            "Val   - Age Loss: 0.0070 | Gender Loss: 0.1591 | Gender Acc: 0.9400\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.90      0.98      0.94       226\n",
            "      Female       0.98      0.91      0.94       274\n",
            "\n",
            "    accuracy                           0.94       500\n",
            "   macro avg       0.94      0.94      0.94       500\n",
            "weighted avg       0.94      0.94      0.94       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[221   5]\n",
            " [ 25 249]]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:48<00:00,  1.46it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 11/20\n",
            "Train - Age Loss: 0.0092 | Gender Loss: 0.1501 | Gender Acc: 0.9396\n",
            "Val   - Age Loss: 0.0068 | Gender Loss: 0.1094 | Gender Acc: 0.9520\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.93      0.97      0.95       226\n",
            "      Female       0.97      0.94      0.96       274\n",
            "\n",
            "    accuracy                           0.95       500\n",
            "   macro avg       0.95      0.95      0.95       500\n",
            "weighted avg       0.95      0.95      0.95       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[219   7]\n",
            " [ 17 257]]\n",
            "ğŸ’¾ Saved best model to face_age_gender_model.pth (Gender Acc: 0.9520)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:48<00:00,  1.46it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 12/20\n",
            "Train - Age Loss: 0.0088 | Gender Loss: 0.1423 | Gender Acc: 0.9369\n",
            "Val   - Age Loss: 0.0066 | Gender Loss: 0.0940 | Gender Acc: 0.9620\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.94      0.98      0.96       226\n",
            "      Female       0.98      0.95      0.96       274\n",
            "\n",
            "    accuracy                           0.96       500\n",
            "   macro avg       0.96      0.96      0.96       500\n",
            "weighted avg       0.96      0.96      0.96       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[222   4]\n",
            " [ 15 259]]\n",
            "ğŸ’¾ Saved best model to face_age_gender_model.pth (Gender Acc: 0.9620)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:48<00:00,  1.46it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 13/20\n",
            "Train - Age Loss: 0.0085 | Gender Loss: 0.1359 | Gender Acc: 0.9471\n",
            "Val   - Age Loss: 0.0063 | Gender Loss: 0.0687 | Gender Acc: 0.9740\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.98      0.96      0.97       226\n",
            "      Female       0.97      0.99      0.98       274\n",
            "\n",
            "    accuracy                           0.97       500\n",
            "   macro avg       0.97      0.97      0.97       500\n",
            "weighted avg       0.97      0.97      0.97       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[217   9]\n",
            " [  4 270]]\n",
            "ğŸ’¾ Saved best model to face_age_gender_model.pth (Gender Acc: 0.9740)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:48<00:00,  1.48it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 14/20\n",
            "Train - Age Loss: 0.0085 | Gender Loss: 0.1175 | Gender Acc: 0.9507\n",
            "Val   - Age Loss: 0.0068 | Gender Loss: 0.0872 | Gender Acc: 0.9560\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       1.00      0.91      0.95       226\n",
            "      Female       0.93      1.00      0.96       274\n",
            "\n",
            "    accuracy                           0.96       500\n",
            "   macro avg       0.96      0.95      0.96       500\n",
            "weighted avg       0.96      0.96      0.96       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[205  21]\n",
            " [  1 273]]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 15/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:47<00:00,  1.49it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 15/20\n",
            "Train - Age Loss: 0.0082 | Gender Loss: 0.1075 | Gender Acc: 0.9567\n",
            "Val   - Age Loss: 0.0071 | Gender Loss: 0.1057 | Gender Acc: 0.9500\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       1.00      0.89      0.94       226\n",
            "      Female       0.92      1.00      0.96       274\n",
            "\n",
            "    accuracy                           0.95       500\n",
            "   macro avg       0.96      0.95      0.95       500\n",
            "weighted avg       0.95      0.95      0.95       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[202  24]\n",
            " [  1 273]]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 16/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:47<00:00,  1.48it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 16/20\n",
            "Train - Age Loss: 0.0083 | Gender Loss: 0.0938 | Gender Acc: 0.9642\n",
            "Val   - Age Loss: 0.0062 | Gender Loss: 0.0934 | Gender Acc: 0.9620\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.98      0.93      0.96       226\n",
            "      Female       0.95      0.99      0.97       274\n",
            "\n",
            "    accuracy                           0.96       500\n",
            "   macro avg       0.96      0.96      0.96       500\n",
            "weighted avg       0.96      0.96      0.96       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[211  15]\n",
            " [  4 270]]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 17/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:47<00:00,  1.48it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 17/20\n",
            "Train - Age Loss: 0.0077 | Gender Loss: 0.0931 | Gender Acc: 0.9602\n",
            "Val   - Age Loss: 0.0058 | Gender Loss: 0.0401 | Gender Acc: 0.9840\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.99      0.97      0.98       226\n",
            "      Female       0.98      0.99      0.99       274\n",
            "\n",
            "    accuracy                           0.98       500\n",
            "   macro avg       0.98      0.98      0.98       500\n",
            "weighted avg       0.98      0.98      0.98       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[220   6]\n",
            " [  2 272]]\n",
            "ğŸ’¾ Saved best model to face_age_gender_model.pth (Gender Acc: 0.9840)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:48<00:00,  1.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 18/20\n",
            "Train - Age Loss: 0.0078 | Gender Loss: 0.0968 | Gender Acc: 0.9636\n",
            "Val   - Age Loss: 0.0073 | Gender Loss: 0.0476 | Gender Acc: 0.9860\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       1.00      0.97      0.98       226\n",
            "      Female       0.98      1.00      0.99       274\n",
            "\n",
            "    accuracy                           0.99       500\n",
            "   macro avg       0.99      0.98      0.99       500\n",
            "weighted avg       0.99      0.99      0.99       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[220   6]\n",
            " [  1 273]]\n",
            "ğŸ’¾ Saved best model to face_age_gender_model.pth (Gender Acc: 0.9860)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:48<00:00,  1.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 19/20\n",
            "Train - Age Loss: 0.0075 | Gender Loss: 0.0873 | Gender Acc: 0.9664\n",
            "Val   - Age Loss: 0.0062 | Gender Loss: 0.0274 | Gender Acc: 0.9920\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.99      0.99      0.99       226\n",
            "      Female       0.99      0.99      0.99       274\n",
            "\n",
            "    accuracy                           0.99       500\n",
            "   macro avg       0.99      0.99      0.99       500\n",
            "weighted avg       0.99      0.99      0.99       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[224   2]\n",
            " [  2 272]]\n",
            "ğŸ’¾ Saved best model to face_age_gender_model.pth (Gender Acc: 0.9920)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:48<00:00,  1.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 20/20\n",
            "Train - Age Loss: 0.0072 | Gender Loss: 0.0782 | Gender Acc: 0.9744\n",
            "Val   - Age Loss: 0.0053 | Gender Loss: 0.0266 | Gender Acc: 0.9900\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.99      0.99      0.99       226\n",
            "      Female       0.99      0.99      0.99       274\n",
            "\n",
            "    accuracy                           0.99       500\n",
            "   macro avg       0.99      0.99      0.99       500\n",
            "weighted avg       0.99      0.99      0.99       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[223   3]\n",
            " [  2 272]]\n",
            "âœ… Loaded pretrained age/gender model\n",
            "\n",
            "ğŸ” Processing Two girls.jpg...\n",
            "\n",
            "0: 480x640 2 faces, 11.7ms\n",
            "Speed: 2.3ms preprocess, 11.7ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "âœ… Found 1 detections in 0.16s\n",
            "1. female (Age: 41.3, Conf: 0.82)\n",
            "ğŸ’¾ Saved visualization to output_Two girls.jpg\n",
            "\n",
            "ğŸ” Processing download.jpg...\n",
            "\n",
            "0: 448x640 23 faces, 11.1ms\n",
            "Speed: 3.5ms preprocess, 11.1ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
            "âŒ No valid detections found\n",
            "\n",
            "ğŸ” Processing how-to-be-a-people-person-1662995088.jpg...\n",
            "\n",
            "0: 352x640 5 faces, 9.8ms\n",
            "Speed: 2.9ms preprocess, 9.8ms inference, 1.6ms postprocess per image at shape (1, 3, 352, 640)\n",
            "âœ… Found 5 detections in 0.09s\n",
            "1. female (Age: 36.2, Conf: 0.74)\n",
            "2. male (Age: 32.4, Conf: 0.79)\n",
            "3. male (Age: 34.4, Conf: 0.72)\n",
            "4. female (Age: 34.1, Conf: 0.77)\n",
            "5. female (Age: 26.2, Conf: 0.80)\n",
            "ğŸ’¾ Saved visualization to output_how-to-be-a-people-person-1662995088.jpg\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}