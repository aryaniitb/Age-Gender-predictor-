{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FFNcY3WUEVE",
        "outputId": "90dcbe63-b719-4310-9e58-a4aff7864b81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "epyJ__2TUn6V"
      },
      "outputs": [],
      "source": [
        "FACE_DATASET_PATH = \"/content/drive/MyDrive/crop_part1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxWbfgtTUt04",
        "outputId": "23b11001-c9ea-48a6-cf55-73f66fd35193"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.171-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.11/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.12.0.88)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.16.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.7.14)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m117.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m86.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m102.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics-8.3.171-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ultralytics-thop, ultralytics\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 ultralytics-8.3.171 ultralytics-thop-2.0.14\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision ultralytics opencv-python-headless numpy pillow requests tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RomPCJw3UxxV"
      },
      "outputs": [],
      "source": [
        "!wget -q \\\n",
        "  https://huggingface.co/ultralytics/yolov8n-face/resolve/main/yolov8n-face.pt \\\n",
        "  -O yolov8n-face.pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8VPLAcCU1pV",
        "outputId": "b8f7a5f9-0641-41af-9f30-41de8de8735b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1qcr9DbgsX3ryrz2uU8w4Xm3cOrRywXqb\n",
            "To: /content/yolov8n-face.pt\n",
            "100% 6.39M/6.39M [00:00<00:00, 18.4MB/s]\n"
          ]
        }
      ],
      "source": [
        "# install gdown if you don't already have it\n",
        "!pip install --quiet gdown\n",
        "\n",
        "# download the YOLOv8-n face model\n",
        "!gdown \"https://drive.google.com/uc?id=1qcr9DbgsX3ryrz2uU8w4Xm3cOrRywXqb\" -O /content/yolov8n-face.pt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torchvision import transforms, models\n",
        "from ultralytics import YOLO\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import glob\n",
        "import re\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, mean_absolute_error\n",
        "\n",
        "# â”€â”€â”€ Configurations â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "DATA_DIR = \"/content/drive/MyDrive/crop_part1\"\n",
        "MODEL_PATH = \"face_age_gender_best.pth\"\n",
        "MAX_AGE = 100.0\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 30\n",
        "EARLY_STOPPING_PATIENCE = 5\n",
        "AGE_LOSS_WEIGHT = 0.7  # Weight for age loss\n",
        "GENDER_LOSS_WEIGHT = 0.3  # Weight for gender loss\n",
        "\n",
        "# â”€â”€â”€ Dataset + Split â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "class FaceDataset(Dataset):\n",
        "    def __init__(self, samples, transform=None):\n",
        "        self.samples = samples\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        p, age, gender = self.samples[idx]\n",
        "        try:\n",
        "            img = Image.open(p).convert(\"RGB\")\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "            return img, age / MAX_AGE, gender\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {p}: {e}\")\n",
        "            # Return dummy data for corrupted images\n",
        "            dummy_img = Image.new('RGB', (224, 224), (0, 0, 0))\n",
        "            if self.transform:\n",
        "                dummy_img = self.transform(dummy_img)\n",
        "            return dummy_img, 0.0, 0\n",
        "\n",
        "# Collect all samples\n",
        "paths = glob.glob(os.path.join(DATA_DIR, \"**\", \"*.jpg\"), recursive=True)\n",
        "paths += glob.glob(os.path.join(DATA_DIR, \"**\", \"*.jpeg\"), recursive=True)\n",
        "paths += glob.glob(os.path.join(DATA_DIR, \"**\", \"*.png\"), recursive=True)\n",
        "samples = []\n",
        "for p in paths:\n",
        "    fn = os.path.basename(p)\n",
        "    m = re.match(r\"(\\d+)_([01])_.*\\.(?:jpg|jpeg|png)\", fn)\n",
        "    if m:\n",
        "        try:\n",
        "            age = min(int(m.group(1)), MAX_AGE)\n",
        "            gender = int(m.group(2))\n",
        "            samples.append((p, age, gender))\n",
        "        except ValueError:\n",
        "            continue\n",
        "\n",
        "# Print class distribution\n",
        "genders = [g for _, _, g in samples]\n",
        "print(f\"Male samples: {genders.count(0)}, Female samples: {genders.count(1)}\")\n",
        "\n",
        "# train-val-test split stratified by gender\n",
        "train_val, test = train_test_split(\n",
        "    samples, test_size=0.1, stratify=genders, random_state=42\n",
        ")\n",
        "train_genders = [g for _, _, g in train_val]\n",
        "train, val = train_test_split(\n",
        "    train_val, test_size=0.2, stratify=train_genders, random_state=42\n",
        ")\n",
        "print(f\"Dataset: train={len(train)}, val={len(val)}, test={len(test)}\")\n",
        "\n",
        "# Transforms\n",
        "tfm_train = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(0.2, 0.2, 0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "tfm_val = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Datasets\n",
        "train_ds = FaceDataset(train, transform=tfm_train)\n",
        "val_ds = FaceDataset(val, transform=tfm_val)\n",
        "test_ds = FaceDataset(test, transform=tfm_val)\n",
        "\n",
        "# Weighted sampler to address class imbalance\n",
        "labels = [g for _, _, g in train]\n",
        "class_sample_count = np.array([labels.count(0), labels.count(1)])\n",
        "weight = 1. / class_sample_count\n",
        "samples_weight = np.array([weight[g] for g in labels])\n",
        "\n",
        "# Use subset sampling to avoid memory issues\n",
        "sampler = WeightedRandomSampler(\n",
        "    samples_weight,\n",
        "    min(2 * len(samples_weight), len(samples_weight)),  # Sample subset\n",
        "    replacement=True\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=sampler, num_workers=4, pin_memory=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "# â”€â”€â”€ Model Definition â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "class AgeGenderModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        base = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
        "\n",
        "        # Extract all layers except the last fully connected layer\n",
        "        self.backbone = nn.Sequential(*list(base.children())[:-2])\n",
        "\n",
        "        # Get feature dimension\n",
        "        with torch.no_grad():\n",
        "            dummy = torch.randn(1, 3, 224, 224)\n",
        "            feat_dim = self.backbone(dummy).shape[1]\n",
        "\n",
        "        self.att = nn.Sequential(\n",
        "            nn.Conv2d(feat_dim, feat_dim // 8, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(feat_dim // 8, 1, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        # Age prediction head\n",
        "        self.age_head = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(feat_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # Gender prediction head\n",
        "        self.gender_head = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(feat_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        f = self.backbone(x)\n",
        "        attn = self.att(f)\n",
        "        f_att = f * attn\n",
        "        pooled = self.pool(f_att)\n",
        "        return self.age_head(pooled), self.gender_head(pooled)\n",
        "\n",
        "# Instantiate\n",
        "model = AgeGenderModel().to(DEVICE)\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "# Losses\n",
        "age_loss_fn = nn.L1Loss()\n",
        "gender_loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Optimizer + scheduler\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
        ")\n",
        "\n",
        "# â”€â”€â”€ Training Loop with Early Stopping â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "best_val_loss = float('inf')\n",
        "patience_counter = 0\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    age_train_loss = 0.0\n",
        "    gender_train_loss = 0.0\n",
        "\n",
        "    for imgs, ages, genders in tqdm(train_loader, desc=f\"Train {epoch}/{EPOCHS}\"):\n",
        "        # Transfer to device\n",
        "        imgs = imgs.to(DEVICE, non_blocking=True)\n",
        "        ages = ages.to(DEVICE, non_blocking=True)\n",
        "        genders = genders.unsqueeze(1).float().to(DEVICE, non_blocking=True)\n",
        "\n",
        "        # Zero gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        age_pred, gender_pred = model(imgs)\n",
        "\n",
        "        # Calculate losses\n",
        "        loss_age = age_loss_fn(age_pred.squeeze(), ages)\n",
        "        loss_gender = gender_loss_fn(gender_pred, genders)\n",
        "        loss = AGE_LOSS_WEIGHT * loss_age + GENDER_LOSS_WEIGHT * loss_gender\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate losses\n",
        "        train_loss += loss.item()\n",
        "        age_train_loss += loss_age.item()\n",
        "        gender_train_loss += loss_gender.item()\n",
        "\n",
        "    # Average training losses\n",
        "    train_loss /= len(train_loader)\n",
        "    age_train_loss /= len(train_loader)\n",
        "    gender_train_loss /= len(train_loader)\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    age_val_loss = 0.0\n",
        "    gender_val_loss = 0.0\n",
        "    all_ages_pred = []\n",
        "    all_ages_true = []\n",
        "    all_genders_pred = []\n",
        "    all_genders_true = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, ages, genders in val_loader:\n",
        "            imgs = imgs.to(DEVICE, non_blocking=True)\n",
        "            ages = ages.to(DEVICE, non_blocking=True)\n",
        "            genders = genders.unsqueeze(1).float().to(DEVICE, non_blocking=True)\n",
        "\n",
        "            # Forward pass\n",
        "            age_pred, gender_pred = model(imgs)\n",
        "\n",
        "            # Calculate losses\n",
        "            loss_age = age_loss_fn(age_pred.squeeze(), ages)\n",
        "            loss_gender = gender_loss_fn(gender_pred, genders)\n",
        "            loss = AGE_LOSS_WEIGHT * loss_age + GENDER_LOSS_WEIGHT * loss_gender\n",
        "\n",
        "            # Accumulate losses\n",
        "            val_loss += loss.item()\n",
        "            age_val_loss += loss_age.item()\n",
        "            gender_val_loss += loss_gender.item()\n",
        "\n",
        "            # Collect predictions for metrics\n",
        "            all_ages_pred.extend((age_pred.squeeze() * MAX_AGE).cpu().numpy())\n",
        "            all_ages_true.extend((ages * MAX_AGE).cpu().numpy())\n",
        "\n",
        "            gender_pred_class = (torch.sigmoid(gender_pred) > 0.5).float()\n",
        "            all_genders_pred.extend(gender_pred_class.cpu().numpy().flatten())\n",
        "            all_genders_true.extend(genders.cpu().numpy().flatten())\n",
        "\n",
        "    # Average validation losses\n",
        "    val_loss /= len(val_loader)\n",
        "    age_val_loss /= len(val_loader)\n",
        "    gender_val_loss /= len(val_loader)\n",
        "\n",
        "    # Calculate metrics\n",
        "    age_mae = mean_absolute_error(all_ages_true, all_ages_pred)\n",
        "    gender_acc = np.mean(np.array(all_genders_pred) == np.array(all_genders_true))\n",
        "\n",
        "    print(f\"\\nEpoch {epoch}/{EPOCHS}\")\n",
        "    print(f\"Train Loss: {train_loss:.4f} | Age: {age_train_loss:.4f} | Gender: {gender_train_loss:.4f}\")\n",
        "    print(f\"Val Loss:   {val_loss:.4f} | Age: {age_val_loss:.4f} | Gender: {gender_val_loss:.4f}\")\n",
        "    print(f\"Val Age MAE: {age_mae:.2f} years | Gender Acc: {gender_acc:.4f}\")\n",
        "\n",
        "    # Check for improvement\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        patience_counter = 0\n",
        "        torch.save(model.state_dict(), MODEL_PATH)\n",
        "        print(f\"ğŸ’¾ Saved best model (Loss: {val_loss:.4f})\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        print(f\"â³ No improvement ({patience_counter}/{EARLY_STOPPING_PATIENCE})\")\n",
        "\n",
        "    # Early stopping check\n",
        "    if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
        "        print(\"ğŸ›‘ Early stopping triggered\")\n",
        "        break\n",
        "\n",
        "    # Update learning rate\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "# â”€â”€â”€ Test Evaluation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "print(\"\\nğŸ”¥ Testing on holdout set...\")\n",
        "model.load_state_dict(torch.load(MODEL_PATH))\n",
        "model.eval()\n",
        "\n",
        "test_age_mae = 0.0\n",
        "test_gender_acc = 0.0\n",
        "all_test_ages_pred = []\n",
        "all_test_ages_true = []\n",
        "all_test_genders_pred = []\n",
        "all_test_genders_true = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, ages, genders in test_loader:\n",
        "        imgs = imgs.to(DEVICE)\n",
        "        ages = ages.to(DEVICE)\n",
        "        genders = genders.unsqueeze(1).float().to(DEVICE)\n",
        "\n",
        "        age_pred, gender_pred = model(imgs)\n",
        "\n",
        "        # Collect predictions\n",
        "        all_test_ages_pred.extend((age_pred.squeeze() * MAX_AGE).cpu().numpy())\n",
        "        all_test_ages_true.extend((ages * MAX_AGE).cpu().numpy())\n",
        "\n",
        "        gender_pred_class = (torch.sigmoid(gender_pred) > 0.5).float()\n",
        "        all_test_genders_pred.extend(gender_pred_class.cpu().numpy().flatten())\n",
        "        all_test_genders_true.extend(genders.cpu().numpy().flatten())\n",
        "\n",
        "# Calculate test metrics\n",
        "test_age_mae = mean_absolute_error(all_test_ages_true, all_test_ages_pred)\n",
        "test_gender_acc = np.mean(np.array(all_test_genders_pred) == np.array(all_test_genders_true))\n",
        "\n",
        "print(f\"Test Age MAE: {test_age_mae:.2f} years\")\n",
        "print(f\"Test Gender Acc: {test_gender_acc:.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(all_test_genders_true, all_test_genders_pred, target_names=[\"Male\", \"Female\"]))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(all_test_genders_true, all_test_genders_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6MEU5Gd8WvmD",
        "outputId": "725cb12f-0b79-4878-8fd6-3d7ae5455dc2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file âœ… \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "Using device: cuda\n",
            "Male samples: 3152, Female samples: 4005\n",
            "Dataset: train=5152, val=1289, test=716\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97.8M/97.8M [00:00<00:00, 198MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model parameters: 24,820,035\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train 1/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 81/81 [03:43<00:00,  2.77s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/30\n",
            "Train Loss: 0.2643 | Age: 0.0912 | Gender: 0.6682\n",
            "Val Loss:   0.2151 | Age: 0.0627 | Gender: 0.5707\n",
            "Val Age MAE: 6.15 years | Gender Acc: 0.7106\n",
            "ğŸ’¾ Saved best model (Loss: 0.2151)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain 2/30:   0%|          | 0/81 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Train 2/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 81/81 [00:51<00:00,  1.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2/30\n",
            "Train Loss: 0.1797 | Age: 0.0608 | Gender: 0.4570\n",
            "Val Loss:   0.1533 | Age: 0.0576 | Gender: 0.3765\n",
            "Val Age MAE: 5.66 years | Gender Acc: 0.8278\n",
            "ğŸ’¾ Saved best model (Loss: 0.1533)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain 3/30:   0%|          | 0/81 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Train 3/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 81/81 [00:51<00:00,  1.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3/30\n",
            "Train Loss: 0.1328 | Age: 0.0558 | Gender: 0.3124\n",
            "Val Loss:   0.1466 | Age: 0.0556 | Gender: 0.3591\n",
            "Val Age MAE: 5.33 years | Gender Acc: 0.8247\n",
            "ğŸ’¾ Saved best model (Loss: 0.1466)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain 4/30:   0%|          | 0/81 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Train 4/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 81/81 [00:51<00:00,  1.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 4/30\n",
            "Train Loss: 0.1174 | Age: 0.0517 | Gender: 0.2706\n",
            "Val Loss:   0.1411 | Age: 0.0531 | Gender: 0.3466\n",
            "Val Age MAE: 5.12 years | Gender Acc: 0.8285\n",
            "ğŸ’¾ Saved best model (Loss: 0.1411)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain 5/30:   0%|          | 0/81 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Train 5/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 81/81 [00:52<00:00,  1.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 5/30\n",
            "Train Loss: 0.0983 | Age: 0.0480 | Gender: 0.2155\n",
            "Val Loss:   0.1606 | Age: 0.0531 | Gender: 0.4114\n",
            "Val Age MAE: 5.12 years | Gender Acc: 0.8340\n",
            "â³ No improvement (1/5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain 6/30:   0%|          | 0/81 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Train 6/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 81/81 [00:52<00:00,  1.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 6/30\n",
            "Train Loss: 0.0880 | Age: 0.0465 | Gender: 0.1847\n",
            "Val Loss:   0.1471 | Age: 0.0506 | Gender: 0.3723\n",
            "Val Age MAE: 4.94 years | Gender Acc: 0.8472\n",
            "â³ No improvement (2/5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain 7/30:   0%|          | 0/81 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Train 7/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 81/81 [00:51<00:00,  1.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 7/30\n",
            "Train Loss: 0.0830 | Age: 0.0450 | Gender: 0.1717\n",
            "Val Loss:   0.1392 | Age: 0.0477 | Gender: 0.3528\n",
            "Val Age MAE: 4.65 years | Gender Acc: 0.8363\n",
            "ğŸ’¾ Saved best model (Loss: 0.1392)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain 8/30:   0%|          | 0/81 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Train 8/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 81/81 [00:51<00:00,  1.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 8/30\n",
            "Train Loss: 0.0706 | Age: 0.0416 | Gender: 0.1382\n",
            "Val Loss:   0.1453 | Age: 0.0498 | Gender: 0.3681\n",
            "Val Age MAE: 4.77 years | Gender Acc: 0.8402\n",
            "â³ No improvement (1/5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain 9/30:   0%|          | 0/81 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Train 9/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 81/81 [00:51<00:00,  1.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 9/30\n",
            "Train Loss: 0.0686 | Age: 0.0423 | Gender: 0.1300\n",
            "Val Loss:   0.1467 | Age: 0.0494 | Gender: 0.3738\n",
            "Val Age MAE: 4.65 years | Gender Acc: 0.8503\n",
            "â³ No improvement (2/5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain 10/30:   0%|          | 0/81 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Train 10/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 81/81 [00:51<00:00,  1.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 10/30\n",
            "Train Loss: 0.0616 | Age: 0.0410 | Gender: 0.1097\n",
            "Val Loss:   0.1462 | Age: 0.0482 | Gender: 0.3750\n",
            "Val Age MAE: 4.49 years | Gender Acc: 0.8503\n",
            "â³ No improvement (3/5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain 11/30:   0%|          | 0/81 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Train 11/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 81/81 [00:52<00:00,  1.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 11/30\n",
            "Train Loss: 0.0586 | Age: 0.0392 | Gender: 0.1037\n",
            "Val Loss:   0.1626 | Age: 0.0491 | Gender: 0.4275\n",
            "Val Age MAE: 4.68 years | Gender Acc: 0.8479\n",
            "â³ No improvement (4/5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain 12/30:   0%|          | 0/81 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Train 12/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 81/81 [00:51<00:00,  1.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 12/30\n",
            "Train Loss: 0.0473 | Age: 0.0378 | Gender: 0.0694\n",
            "Val Loss:   0.1717 | Age: 0.0460 | Gender: 0.4648\n",
            "Val Age MAE: 4.39 years | Gender Acc: 0.8503\n",
            "â³ No improvement (5/5)\n",
            "ğŸ›‘ Early stopping triggered\n",
            "\n",
            "ğŸ”¥ Testing on holdout set...\n",
            "Test Age MAE: 4.56 years\n",
            "Test Gender Acc: 0.8478\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.81      0.85      0.83       315\n",
            "      Female       0.88      0.85      0.86       401\n",
            "\n",
            "    accuracy                           0.85       716\n",
            "   macro avg       0.85      0.85      0.85       716\n",
            "weighted avg       0.85      0.85      0.85       716\n",
            "\n",
            "Confusion Matrix:\n",
            "[[267  48]\n",
            " [ 61 340]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€â”€ Imports and Configurations â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torchvision import transforms, models\n",
        "from ultralytics import YOLO\n",
        "import torch.nn as nn\n",
        "import os\n",
        "from sklearn.metrics import confusion_matrix, classification_report, mean_absolute_error\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "MODEL_PATH = \"face_age_gender_best.pth\"\n",
        "MAX_AGE = 100.0\n",
        "\n",
        "tfm_val = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# â”€â”€â”€ Model Definition â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "class AgeGenderModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        base = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
        "\n",
        "        # Extract all layers except the last fully connected layer\n",
        "        self.backbone = nn.Sequential(*list(base.children())[:-2])\n",
        "\n",
        "        # Get feature dimension\n",
        "        with torch.no_grad():\n",
        "            dummy = torch.randn(1, 3, 224, 224)\n",
        "            feat_dim = self.backbone(dummy).shape[1]\n",
        "\n",
        "        self.att = nn.Sequential(\n",
        "            nn.Conv2d(feat_dim, feat_dim // 8, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(feat_dim // 8, 1, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        # Age prediction head\n",
        "        self.age_head = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(feat_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # Gender prediction head\n",
        "        self.gender_head = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(feat_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        f = self.backbone(x)\n",
        "        attn = self.att(f)\n",
        "        f_att = f * attn\n",
        "        pooled = self.pool(f_att)\n",
        "        return self.age_head(pooled), self.gender_head(pooled)\n",
        "\n",
        "\n",
        "# â”€â”€â”€ Face Detection And Attribute Prediction â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def predict_age_gender(image_path, min_face_size=30, min_confidence=0.3):\n",
        "    \"\"\"\n",
        "    Detect faces in an image and predict age/gender for each face.\n",
        "\n",
        "    Args:\n",
        "        image_path (str): Path to input image\n",
        "        min_face_size (int): Minimum face dimension to consider (pixels)\n",
        "        min_confidence (float): Minimum confidence threshold for detection\n",
        "\n",
        "    Returns:\n",
        "        list: List of detection dictionaries with attributes\n",
        "        Image: Annotated image\n",
        "    \"\"\"\n",
        "    # Load image\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        print(f\"âŒ Could not read image: {image_path}\")\n",
        "        return [], None\n",
        "\n",
        "    # Convert to RGB (YOLO expects RGB)\n",
        "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    orig_img = img.copy()  # Keep a copy for visualization\n",
        "\n",
        "    # Initialize YOLO face detector\n",
        "    face_detector = YOLO(\"yolov8n-face.pt\")\n",
        "\n",
        "    # Run face detection\n",
        "    detections = []\n",
        "    results = face_detector(img_rgb, conf=0.5)  # Use confidence threshold 0.5\n",
        "\n",
        "    # Process each detected face\n",
        "    for result in results:\n",
        "        if hasattr(result, 'boxes') and result.boxes is not None:\n",
        "            for box in result.boxes:\n",
        "                if int(box.cls) == 0:  # Only process face detections\n",
        "                    # Get bounding box coordinates\n",
        "                    x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())\n",
        "                    conf = box.conf.item()\n",
        "\n",
        "                    # Filter small faces\n",
        "                    w, h = x2 - x1, y2 - y1\n",
        "                    if w < min_face_size or h < min_face_size:\n",
        "                        continue\n",
        "\n",
        "                    # Crop face\n",
        "                    face_roi = img_rgb[y1:y2, x1:x2]\n",
        "                    if face_roi.size == 0:\n",
        "                        continue\n",
        "\n",
        "                    # Convert to PIL Image for transformation\n",
        "                    face_pil = Image.fromarray(face_roi)\n",
        "\n",
        "                    # Apply validation transform\n",
        "                    face_tensor = tfm_val(face_pil).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "                    # Predict age and gender\n",
        "                    with torch.no_grad():\n",
        "                        age_pred, gender_pred = model(face_tensor)\n",
        "\n",
        "                        # Process predictions\n",
        "                        age = age_pred[0][0].item() * MAX_AGE\n",
        "                        gender_prob = torch.sigmoid(gender_pred[0][0]).item()\n",
        "\n",
        "                        # Determine gender\n",
        "                        if gender_prob > 0.5:\n",
        "                            gender = \"Female\"\n",
        "                            gender_conf = gender_prob\n",
        "                        else:\n",
        "                            gender = \"Male\"\n",
        "                            gender_conf = 1 - gender_prob\n",
        "\n",
        "                        # Combined confidence\n",
        "                        combined_conf = (conf + gender_conf) / 2\n",
        "\n",
        "                        if combined_conf >= min_confidence:\n",
        "                            detections.append({\n",
        "                                \"bbox\": (x1, y1, x2, y2),\n",
        "                                \"age\": age,\n",
        "                                \"gender\": gender,\n",
        "                                \"gender_confidence\": gender_conf,\n",
        "                                \"face_confidence\": conf,\n",
        "                                \"combined_confidence\": combined_conf\n",
        "                            })\n",
        "\n",
        "    # Visualize results\n",
        "    annotated_img = visualize_predictions(orig_img, detections)\n",
        "\n",
        "    return detections, annotated_img\n",
        "\n",
        "def visualize_predictions(image, detections):\n",
        "    \"\"\"\n",
        "    Draw bounding boxes and labels on image\n",
        "\n",
        "    Args:\n",
        "        image (np.array): Original image (BGR format)\n",
        "        detections (list): List of detection dictionaries\n",
        "\n",
        "    Returns:\n",
        "        np.array: Annotated image\n",
        "    \"\"\"\n",
        "    for det in detections:\n",
        "        x1, y1, x2, y2 = det[\"bbox\"]\n",
        "        age = det[\"age\"]\n",
        "        gender = det[\"gender\"]\n",
        "        conf = det[\"combined_confidence\"]\n",
        "\n",
        "        # Choose color based on gender\n",
        "        color = (0, 0, 255) if gender == \"Female\" else (255, 0, 0)  # Red=F, Blue=M\n",
        "\n",
        "        # Draw bounding box\n",
        "        cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)\n",
        "\n",
        "        # Create label\n",
        "        label = f\"{gender} {age:.1f}y ({conf:.2f})\"\n",
        "\n",
        "        # Calculate text size for background\n",
        "        (text_width, text_height), _ = cv2.getTextSize(\n",
        "            label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2\n",
        "        )\n",
        "\n",
        "        # Draw text background\n",
        "        cv2.rectangle(\n",
        "            image,\n",
        "            (x1, y1 - text_height - 10),\n",
        "            (x1 + text_width, y1 - 10),\n",
        "            color,\n",
        "            -1\n",
        "        )\n",
        "\n",
        "        # Draw text\n",
        "        cv2.putText(\n",
        "            image, label, (x1, y1 - 10),\n",
        "            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2\n",
        "        )\n",
        "\n",
        "    return image\n",
        "\n",
        "# â”€â”€â”€ Inference On Sample Images â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Load your trained model\n",
        "model = AgeGenderModel().to(DEVICE)\n",
        "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
        "model.eval()\n",
        "print(\"âœ… Loaded trained age/gender model\")\n",
        "\n",
        "# Define sample images for testing\n",
        "TEST_IMAGES = [\"/content/Family.jpg\"]\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(\"outputs\", exist_ok=True)\n",
        "\n",
        "# Process each test image\n",
        "for img_path in TEST_IMAGES:\n",
        "    if not os.path.exists(img_path):\n",
        "        print(f\"âŒ Image not found: {img_path}\")\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nğŸ” Processing: {os.path.basename(img_path)}\")\n",
        "\n",
        "    # Run prediction pipeline\n",
        "    detections, annotated_img = predict_age_gender(img_path, min_confidence=0.3)\n",
        "\n",
        "    if detections:\n",
        "        print(f\"âœ… Found {len(detections)} faces:\")\n",
        "        for i, det in enumerate(detections, 1):\n",
        "            print(f\"  {i}. {det['gender']} ({det['age']:.1f}y) \"\n",
        "                  f\"[Face: {det['face_confidence']:.2f}, Gender: {det['gender_confidence']:.2f}]\")\n",
        "\n",
        "        # Save annotated image\n",
        "        output_path = os.path.join(\"outputs\", f\"detected_{os.path.basename(img_path)}\")\n",
        "        cv2.imwrite(output_path, annotated_img)\n",
        "        print(f\"ğŸ’¾ Saved annotated image to: {output_path}\")\n",
        "\n",
        "        # Display in Colab\n",
        "        from IPython.display import Image, display\n",
        "        display(Image(filename=output_path))\n",
        "    else:\n",
        "        print(\"âŒ No faces detected\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588
        },
        "id": "_F71W0XNcIwA",
        "outputId": "7d589a0e-59c0-485d-eae6-3452417deefa"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Loaded trained age/gender model\n",
            "\n",
            "ğŸ” Processing: Family.jpg\n",
            "\n",
            "0: 448x640 3 faces, 79.1ms\n",
            "Speed: 4.8ms preprocess, 79.1ms inference, 2.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "âœ… Found 3 faces:\n",
            "  1. Male (46.6y) [Face: 0.85, Gender: 1.00]\n",
            "  2. Female (7.4y) [Face: 0.84, Gender: 0.97]\n",
            "  3. Female (13.1y) [Face: 0.80, Gender: 0.85]\n",
            "ğŸ’¾ Saved annotated image to: outputs/detected_Family.jpg\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/jpeg": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQKRFToeU7Ew",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0847c542-e3cd-45c1-8ee4-d1fefee6cc4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file âœ… \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.23M/6.23M [00:00<00:00, 112MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ” Loading dataset...\n",
            "Loaded 5000 face images\n",
            "âœ… Loaded dataset with 5000 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97.8M/97.8M [00:00<00:00, 189MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 4500 face images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 500 face images\n",
            "ğŸš€ Training on 4500 samples, validating on 500 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [04:30<00:00,  3.82s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/20\n",
            "Train - Age Loss: 0.0311 | Gender Loss: 0.6478 | Gender Acc: 0.6124\n",
            "Val   - Age Loss: 0.0188 | Gender Loss: 0.5487 | Gender Acc: 0.7260\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.79      0.54      0.64       226\n",
            "      Female       0.70      0.88      0.78       274\n",
            "\n",
            "    accuracy                           0.73       500\n",
            "   macro avg       0.74      0.71      0.71       500\n",
            "weighted avg       0.74      0.73      0.72       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[121 105]\n",
            " [ 32 242]]\n",
            "ğŸ’¾ Saved best model to face_age_gender_model.pth (Gender Acc: 0.7260)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:50<00:00,  1.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2/20\n",
            "Train - Age Loss: 0.0199 | Gender Loss: 0.4722 | Gender Acc: 0.7689\n",
            "Val   - Age Loss: 0.0111 | Gender Loss: 0.3583 | Gender Acc: 0.8160\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.91      0.66      0.76       226\n",
            "      Female       0.77      0.95      0.85       274\n",
            "\n",
            "    accuracy                           0.82       500\n",
            "   macro avg       0.84      0.80      0.81       500\n",
            "weighted avg       0.83      0.82      0.81       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[149  77]\n",
            " [ 15 259]]\n",
            "ğŸ’¾ Saved best model to face_age_gender_model.pth (Gender Acc: 0.8160)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:50<00:00,  1.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3/20\n",
            "Train - Age Loss: 0.0153 | Gender Loss: 0.3741 | Gender Acc: 0.8202\n",
            "Val   - Age Loss: 0.0099 | Gender Loss: 0.3104 | Gender Acc: 0.8580\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.82      0.87      0.85       226\n",
            "      Female       0.89      0.85      0.87       274\n",
            "\n",
            "    accuracy                           0.86       500\n",
            "   macro avg       0.86      0.86      0.86       500\n",
            "weighted avg       0.86      0.86      0.86       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[197  29]\n",
            " [ 42 232]]\n",
            "ğŸ’¾ Saved best model to face_age_gender_model.pth (Gender Acc: 0.8580)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:50<00:00,  1.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 4/20\n",
            "Train - Age Loss: 0.0133 | Gender Loss: 0.3208 | Gender Acc: 0.8533\n",
            "Val   - Age Loss: 0.0079 | Gender Loss: 0.2564 | Gender Acc: 0.8840\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.83      0.93      0.88       226\n",
            "      Female       0.94      0.85      0.89       274\n",
            "\n",
            "    accuracy                           0.88       500\n",
            "   macro avg       0.88      0.89      0.88       500\n",
            "weighted avg       0.89      0.88      0.88       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[210  16]\n",
            " [ 42 232]]\n",
            "ğŸ’¾ Saved best model to face_age_gender_model.pth (Gender Acc: 0.8840)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:49<00:00,  1.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 5/20\n",
            "Train - Age Loss: 0.0118 | Gender Loss: 0.2731 | Gender Acc: 0.8771\n",
            "Val   - Age Loss: 0.0077 | Gender Loss: 0.2210 | Gender Acc: 0.8920\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.86      0.90      0.88       226\n",
            "      Female       0.92      0.88      0.90       274\n",
            "\n",
            "    accuracy                           0.89       500\n",
            "   macro avg       0.89      0.89      0.89       500\n",
            "weighted avg       0.89      0.89      0.89       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[204  22]\n",
            " [ 32 242]]\n",
            "ğŸ’¾ Saved best model to face_age_gender_model.pth (Gender Acc: 0.8920)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:50<00:00,  1.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 6/20\n",
            "Train - Age Loss: 0.0113 | Gender Loss: 0.2526 | Gender Acc: 0.8913\n",
            "Val   - Age Loss: 0.0074 | Gender Loss: 0.2231 | Gender Acc: 0.9260\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.90      0.94      0.92       226\n",
            "      Female       0.95      0.91      0.93       274\n",
            "\n",
            "    accuracy                           0.93       500\n",
            "   macro avg       0.92      0.93      0.93       500\n",
            "weighted avg       0.93      0.93      0.93       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[213  13]\n",
            " [ 24 250]]\n",
            "ğŸ’¾ Saved best model to face_age_gender_model.pth (Gender Acc: 0.9260)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:51<00:00,  1.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 7/20\n",
            "Train - Age Loss: 0.0107 | Gender Loss: 0.2204 | Gender Acc: 0.9029\n",
            "Val   - Age Loss: 0.0071 | Gender Loss: 0.1902 | Gender Acc: 0.9080\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.96      0.83      0.89       226\n",
            "      Female       0.88      0.97      0.92       274\n",
            "\n",
            "    accuracy                           0.91       500\n",
            "   macro avg       0.92      0.90      0.91       500\n",
            "weighted avg       0.91      0.91      0.91       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[188  38]\n",
            " [  8 266]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:50<00:00,  1.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 8/20\n",
            "Train - Age Loss: 0.0098 | Gender Loss: 0.2047 | Gender Acc: 0.9104\n",
            "Val   - Age Loss: 0.0072 | Gender Loss: 0.1470 | Gender Acc: 0.9460\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.93      0.95      0.94       226\n",
            "      Female       0.96      0.95      0.95       274\n",
            "\n",
            "    accuracy                           0.95       500\n",
            "   macro avg       0.95      0.95      0.95       500\n",
            "weighted avg       0.95      0.95      0.95       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[214  12]\n",
            " [ 15 259]]\n",
            "ğŸ’¾ Saved best model to face_age_gender_model.pth (Gender Acc: 0.9460)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:51<00:00,  1.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 9/20\n",
            "Train - Age Loss: 0.0097 | Gender Loss: 0.1826 | Gender Acc: 0.9229\n",
            "Val   - Age Loss: 0.0071 | Gender Loss: 0.1423 | Gender Acc: 0.9440\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.91      0.98      0.94       226\n",
            "      Female       0.98      0.92      0.95       274\n",
            "\n",
            "    accuracy                           0.94       500\n",
            "   macro avg       0.94      0.95      0.94       500\n",
            "weighted avg       0.95      0.94      0.94       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[221   5]\n",
            " [ 23 251]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:50<00:00,  1.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 10/20\n",
            "Train - Age Loss: 0.0096 | Gender Loss: 0.1746 | Gender Acc: 0.9318\n",
            "Val   - Age Loss: 0.0064 | Gender Loss: 0.1031 | Gender Acc: 0.9580\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.96      0.95      0.95       226\n",
            "      Female       0.96      0.96      0.96       274\n",
            "\n",
            "    accuracy                           0.96       500\n",
            "   macro avg       0.96      0.96      0.96       500\n",
            "weighted avg       0.96      0.96      0.96       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[215  11]\n",
            " [ 10 264]]\n",
            "ğŸ’¾ Saved best model to face_age_gender_model.pth (Gender Acc: 0.9580)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:50<00:00,  1.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 11/20\n",
            "Train - Age Loss: 0.0088 | Gender Loss: 0.1574 | Gender Acc: 0.9378\n",
            "Val   - Age Loss: 0.0063 | Gender Loss: 0.0893 | Gender Acc: 0.9700\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.95      0.99      0.97       226\n",
            "      Female       0.99      0.96      0.97       274\n",
            "\n",
            "    accuracy                           0.97       500\n",
            "   macro avg       0.97      0.97      0.97       500\n",
            "weighted avg       0.97      0.97      0.97       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[223   3]\n",
            " [ 12 262]]\n",
            "ğŸ’¾ Saved best model to face_age_gender_model.pth (Gender Acc: 0.9700)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:50<00:00,  1.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 12/20\n",
            "Train - Age Loss: 0.0091 | Gender Loss: 0.1417 | Gender Acc: 0.9413\n",
            "Val   - Age Loss: 0.0069 | Gender Loss: 0.0940 | Gender Acc: 0.9560\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.95      0.96      0.95       226\n",
            "      Female       0.96      0.96      0.96       274\n",
            "\n",
            "    accuracy                           0.96       500\n",
            "   macro avg       0.96      0.96      0.96       500\n",
            "weighted avg       0.96      0.96      0.96       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[216  10]\n",
            " [ 12 262]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:50<00:00,  1.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 13/20\n",
            "Train - Age Loss: 0.0086 | Gender Loss: 0.1141 | Gender Acc: 0.9551\n",
            "Val   - Age Loss: 0.0063 | Gender Loss: 0.0845 | Gender Acc: 0.9660\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.94      0.99      0.96       226\n",
            "      Female       0.99      0.95      0.97       274\n",
            "\n",
            "    accuracy                           0.97       500\n",
            "   macro avg       0.96      0.97      0.97       500\n",
            "weighted avg       0.97      0.97      0.97       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[223   3]\n",
            " [ 14 260]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:52<00:00,  1.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 14/20\n",
            "Train - Age Loss: 0.0085 | Gender Loss: 0.1248 | Gender Acc: 0.9511\n",
            "Val   - Age Loss: 0.0060 | Gender Loss: 0.1114 | Gender Acc: 0.9560\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.92      0.99      0.95       226\n",
            "      Female       0.99      0.93      0.96       274\n",
            "\n",
            "    accuracy                           0.96       500\n",
            "   macro avg       0.95      0.96      0.96       500\n",
            "weighted avg       0.96      0.96      0.96       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[223   3]\n",
            " [ 19 255]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:51<00:00,  1.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 15/20\n",
            "Train - Age Loss: 0.0084 | Gender Loss: 0.1120 | Gender Acc: 0.9564\n",
            "Val   - Age Loss: 0.0054 | Gender Loss: 0.0535 | Gender Acc: 0.9880\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.98      1.00      0.99       226\n",
            "      Female       1.00      0.98      0.99       274\n",
            "\n",
            "    accuracy                           0.99       500\n",
            "   macro avg       0.99      0.99      0.99       500\n",
            "weighted avg       0.99      0.99      0.99       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[225   1]\n",
            " [  5 269]]\n",
            "ğŸ’¾ Saved best model to face_age_gender_model.pth (Gender Acc: 0.9880)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:50<00:00,  1.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 16/20\n",
            "Train - Age Loss: 0.0080 | Gender Loss: 0.0975 | Gender Acc: 0.9591\n",
            "Val   - Age Loss: 0.0052 | Gender Loss: 0.0476 | Gender Acc: 0.9780\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.98      0.97      0.98       226\n",
            "      Female       0.98      0.98      0.98       274\n",
            "\n",
            "    accuracy                           0.98       500\n",
            "   macro avg       0.98      0.98      0.98       500\n",
            "weighted avg       0.98      0.98      0.98       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[220   6]\n",
            " [  5 269]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:48<00:00,  1.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 17/20\n",
            "Train - Age Loss: 0.0076 | Gender Loss: 0.0978 | Gender Acc: 0.9633\n",
            "Val   - Age Loss: 0.0053 | Gender Loss: 0.0458 | Gender Acc: 0.9780\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.97      0.98      0.98       226\n",
            "      Female       0.98      0.98      0.98       274\n",
            "\n",
            "    accuracy                           0.98       500\n",
            "   macro avg       0.98      0.98      0.98       500\n",
            "weighted avg       0.98      0.98      0.98       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[221   5]\n",
            " [  6 268]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:48<00:00,  1.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 18/20\n",
            "Train - Age Loss: 0.0076 | Gender Loss: 0.0848 | Gender Acc: 0.9680\n",
            "Val   - Age Loss: 0.0059 | Gender Loss: 0.0369 | Gender Acc: 0.9820\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.97      0.99      0.98       226\n",
            "      Female       0.99      0.97      0.98       274\n",
            "\n",
            "    accuracy                           0.98       500\n",
            "   macro avg       0.98      0.98      0.98       500\n",
            "weighted avg       0.98      0.98      0.98       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[224   2]\n",
            " [  7 267]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:50<00:00,  1.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 19/20\n",
            "Train - Age Loss: 0.0077 | Gender Loss: 0.0807 | Gender Acc: 0.9689\n",
            "Val   - Age Loss: 0.0052 | Gender Loss: 0.0347 | Gender Acc: 0.9860\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.98      0.99      0.98       226\n",
            "      Female       0.99      0.98      0.99       274\n",
            "\n",
            "    accuracy                           0.99       500\n",
            "   macro avg       0.99      0.99      0.99       500\n",
            "weighted avg       0.99      0.99      0.99       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[224   2]\n",
            " [  5 269]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:48<00:00,  1.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 20/20\n",
            "Train - Age Loss: 0.0074 | Gender Loss: 0.0769 | Gender Acc: 0.9707\n",
            "Val   - Age Loss: 0.0054 | Gender Loss: 0.0390 | Gender Acc: 0.9820\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.99      0.97      0.98       226\n",
            "      Female       0.97      0.99      0.98       274\n",
            "\n",
            "    accuracy                           0.98       500\n",
            "   macro avg       0.98      0.98      0.98       500\n",
            "weighted avg       0.98      0.98      0.98       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[219   7]\n",
            " [  2 272]]\n",
            "âœ… Loaded pretrained age/gender model\n",
            "\n",
            "ğŸ” Processing Two girls.jpg...\n",
            "\n",
            "0: 480x640 2 persons, 43.0ms\n",
            "Speed: 9.6ms preprocess, 43.0ms inference, 327.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 faces, 16.7ms\n",
            "Speed: 1.9ms preprocess, 16.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "âŒ No valid detections found\n",
            "\n",
            "ğŸ” Processing download.jpg...\n",
            "\n",
            "0: 448x640 5 persons, 39.8ms\n",
            "Speed: 2.3ms preprocess, 39.8ms inference, 1.8ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 23 faces, 16.8ms\n",
            "Speed: 2.2ms preprocess, 16.8ms inference, 1.8ms postprocess per image at shape (1, 3, 448, 640)\n",
            "âŒ No valid detections found\n",
            "\n",
            "ğŸ” Processing how-to-be-a-people-person-1662995088.jpg...\n",
            "\n",
            "0: 352x640 5 persons, 57.2ms\n",
            "Speed: 6.0ms preprocess, 57.2ms inference, 2.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 5 faces, 23.3ms\n",
            "Speed: 5.7ms preprocess, 23.3ms inference, 1.7ms postprocess per image at shape (1, 3, 352, 640)\n",
            "âœ… Found 5 detections in 0.21s\n",
            "1. female (Age: 36.2, Conf: 0.68)\n",
            "2. female (Age: 27.7, Conf: 0.75)\n",
            "3. female (Age: 28.7, Conf: 0.73)\n",
            "4. female (Age: 38.7, Conf: 0.82)\n",
            "5. female (Age: 36.9, Conf: 0.69)\n",
            "ğŸ’¾ Saved visualization to output_how-to-be-a-people-person-1662995088.jpg\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torchvision import transforms, models\n",
        "from ultralytics import YOLO\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import glob\n",
        "import re\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import shutil\n",
        "from sklearn.metrics import confusion_matrix, classification_report # Import metrics\n",
        "\n",
        "# Configuration\n",
        "TRAIN_MODEL = True\n",
        "RUN_INFERENCE = True\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Paths\n",
        "FACE_DATASET_PATH = \"/content/drive/MyDrive/crop_part1\"\n",
        "MODEL_PATH = \"face_age_gender_model.pth\"\n",
        "TEST_IMAGES = [\n",
        "    \"/content/Two girls.jpg\",\n",
        "    \"/content/download.jpg\",\n",
        "    \"/content/how-to-be-a-people-person-1662995088.jpg\"\n",
        "]\n",
        "MAX_AGE = 100.0\n",
        "\n",
        "# Download YOLO models if needed\n",
        "if not os.path.exists(\"yolov8n.pt\"):\n",
        "    torch.hub.download_url_to_file(\"https://github.com/ultralytics/assets/releases/download/v8.1.0/yolov8n.pt\", \"yolov8n.pt\")\n",
        "if not os.path.exists(\"yolov8n-face.pt\"):\n",
        "    torch.hub.download_url_to_file(\"https://akanametov.github.io/yolov8-face/yolov8n-face.pt\", \"yolov8n-face.pt\")\n",
        "\n",
        "# â”€â”€â”€ Dataset â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "class FaceAgeGenderDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None, max_samples=None):\n",
        "        self.transform = transform\n",
        "        paths = glob.glob(os.path.join(root_dir, \"**\", \"*.jpg\"), recursive=True)\n",
        "        paths += glob.glob(os.path.join(root_dir, \"**\", \"*.jpeg\"), recursive=True)\n",
        "        paths += glob.glob(os.path.join(root_dir, \"**\", \"*.png\"), recursive=True)\n",
        "        self.samples = []\n",
        "\n",
        "        for p in paths:\n",
        "            fn = os.path.basename(p)\n",
        "            m = re.match(r\"(\\d+)_([01])_.*\\.(?:jpg|jpeg|png)\", fn)\n",
        "            if m:\n",
        "                try:\n",
        "                    age = min(int(m.group(1)), MAX_AGE)\n",
        "                    gender = int(m.group(2))\n",
        "                    self.samples.append((p, age, gender))\n",
        "                    if max_samples and len(self.samples) >= max_samples:\n",
        "                        break\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "        if not self.samples:\n",
        "            raise RuntimeError(\"No valid samples found\")\n",
        "        print(f\"Loaded {len(self.samples)} face images\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        p, age, gender = self.samples[idx]\n",
        "        try:\n",
        "            img = Image.open(p).convert(\"RGB\")\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "            # Normalize age between 0-1\n",
        "            norm_age = age / MAX_AGE\n",
        "            return img, norm_age, gender\n",
        "        except:\n",
        "            # Fallback for corrupted images\n",
        "            img = Image.new(\"RGB\", (224, 224), (0, 0, 0))\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "            return img, 0.0, 0\n",
        "\n",
        "# â”€â”€â”€ Enhanced Model with Attention â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "class AgeGenderModel(nn.Module):\n",
        "    def __init__(self, backbone=\"resnet50\"):\n",
        "        super().__init__()\n",
        "        if backbone == \"resnet34\":\n",
        "            base = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)\n",
        "            feat_dim = 512\n",
        "        else:\n",
        "            base = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
        "            feat_dim = 2048\n",
        "\n",
        "        # Feature extraction\n",
        "        self.features = nn.Sequential(*list(base.children())[:-1])\n",
        "\n",
        "        # Attention mechanism\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Conv2d(feat_dim, feat_dim//8, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(feat_dim//8, 1, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # Age estimation (regression)\n",
        "        self.age_head = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(feat_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid()  # Output between 0-1\n",
        "        )\n",
        "\n",
        "        # Gender classification\n",
        "        self.gender_head = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(feat_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "\n",
        "        # Initialize weights\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.features(x)\n",
        "\n",
        "        # Apply attention\n",
        "        att = self.attention(features)\n",
        "        weighted_features = features * att\n",
        "\n",
        "        # Global pooling\n",
        "        pooled = F.adaptive_avg_pool2d(weighted_features, (1, 1))\n",
        "\n",
        "        # Heads\n",
        "        age = self.age_head(pooled)\n",
        "        gender = self.gender_head(pooled)\n",
        "        return age, gender\n",
        "\n",
        "# â”€â”€â”€ Robust Face Attribute Detector â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "class FaceAttributeDetector:\n",
        "    def __init__(self, model_path=None, backbone=\"resnet50\"):\n",
        "        # Person detector (for full body)\n",
        "        self.person_detector = YOLO(\"yolov8n.pt\")\n",
        "        # Face detector (for precise face detection)\n",
        "        self.face_detector = YOLO(\"yolov8n-face.pt\")\n",
        "\n",
        "        # Age/gender model\n",
        "        self.net = AgeGenderModel(backbone).to(DEVICE)\n",
        "        self.backbone = backbone\n",
        "\n",
        "        # Transformations\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "        # Load model if available\n",
        "        if model_path and os.path.exists(model_path):\n",
        "            try:\n",
        "                self.net.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
        "                print(\"âœ… Loaded pretrained age/gender model\")\n",
        "            except Exception as e:\n",
        "                print(f\"âš ï¸ Failed to load model weights: {e}\")\n",
        "\n",
        "    def train(self, dataset, epochs=20, batch_size=64, save_path=MODEL_PATH):\n",
        "        train_size = int(0.9 * len(dataset))\n",
        "        val_size = len(dataset) - train_size\n",
        "        # Create separate datasets with different transforms\n",
        "        train_set = FaceAgeGenderDataset(\n",
        "            FACE_DATASET_PATH,\n",
        "            transform=transforms.Compose([\n",
        "                transforms.Resize(256),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.RandomRotation(10),\n",
        "                transforms.RandomAffine(0, shear=10),\n",
        "                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "                transforms.RandomCrop(224),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "                transforms.RandomErasing(p=0.3, scale=(0.02, 0.2))\n",
        "            ]),\n",
        "            max_samples=train_size\n",
        "        )\n",
        "        val_set = FaceAgeGenderDataset(\n",
        "            FACE_DATASET_PATH,\n",
        "            transform=transforms.Compose([\n",
        "                transforms.Resize(256),\n",
        "                transforms.CenterCrop(224),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "            ]),\n",
        "            max_samples=val_size\n",
        "        )\n",
        "\n",
        "\n",
        "        train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=min(4, os.cpu_count()))\n",
        "        val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=min(2, os.cpu_count()))\n",
        "\n",
        "        age_loss_fn = nn.MSELoss()\n",
        "        gender_loss_fn = nn.BCEWithLogitsLoss()\n",
        "        optimizer = optim.AdamW(self.net.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
        "\n",
        "        print(f\"ğŸš€ Training on {len(train_set)} samples, validating on {len(val_set)} samples\")\n",
        "        best_val_gender_acc = 0.0\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            self.net.train()\n",
        "            train_age_loss, train_gender_loss = 0.0, 0.0\n",
        "            correct_gender, total_samples = 0, 0\n",
        "\n",
        "            for imgs, ages, genders in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
        "                imgs = imgs.to(DEVICE)\n",
        "                ages = ages.float().to(DEVICE)\n",
        "                genders = genders.float().to(DEVICE).unsqueeze(1)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                age_preds, gender_preds = self.net(imgs)\n",
        "\n",
        "                age_loss = age_loss_fn(age_preds.squeeze(), ages)\n",
        "                gender_loss = gender_loss_fn(gender_preds, genders)\n",
        "                loss = 0.5 * age_loss + 0.5 * gender_loss\n",
        "\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.net.parameters(), 1.0)\n",
        "                optimizer.step()\n",
        "\n",
        "                train_age_loss += age_loss.item()\n",
        "                train_gender_loss += gender_loss.item()\n",
        "\n",
        "                gender_preds_bin = (torch.sigmoid(gender_preds) > 0.5).float()\n",
        "                correct_gender += (gender_preds_bin == genders).sum().item()\n",
        "                total_samples += imgs.size(0)\n",
        "\n",
        "            self.net.eval()\n",
        "            val_age_loss, val_gender_loss = 0.0, 0.0\n",
        "            val_correct_gender, val_total = 0, 0\n",
        "            y_true, y_pred = [], []\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for imgs, ages, genders in val_loader:\n",
        "                    imgs = imgs.to(DEVICE)\n",
        "                    ages = ages.float().to(DEVICE)\n",
        "                    genders = genders.float().to(DEVICE).unsqueeze(1)\n",
        "\n",
        "                    age_preds, gender_preds = self.net(imgs)\n",
        "\n",
        "                    val_age_loss += age_loss_fn(age_preds.squeeze(), ages).item()\n",
        "                    val_gender_loss += gender_loss_fn(gender_preds, genders).item()\n",
        "\n",
        "                    gender_preds_bin = (torch.sigmoid(gender_preds) > 0.5).float()\n",
        "                    val_correct_gender += (gender_preds_bin == genders).sum().item()\n",
        "                    val_total += imgs.size(0)\n",
        "\n",
        "                    y_true.extend(genders.cpu().numpy().flatten())\n",
        "                    y_pred.extend(gender_preds_bin.cpu().numpy().flatten())\n",
        "\n",
        "            train_age_loss /= len(train_loader)\n",
        "            train_gender_loss /= len(train_loader)\n",
        "            val_age_loss /= len(val_loader)\n",
        "            val_gender_loss /= len(val_loader)\n",
        "            train_gender_acc = correct_gender / total_samples\n",
        "            val_gender_acc = val_correct_gender / val_total\n",
        "            val_combined_loss = 0.5 * val_age_loss + 0.5 * val_gender_loss\n",
        "\n",
        "            print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "            print(f\"Train - Age Loss: {train_age_loss:.4f} | Gender Loss: {train_gender_loss:.4f} | Gender Acc: {train_gender_acc:.4f}\")\n",
        "            print(f\"Val   - Age Loss: {val_age_loss:.4f} | Gender Loss: {val_gender_loss:.4f} | Gender Acc: {val_gender_acc:.4f}\")\n",
        "            print(\"Classification Report:\")\n",
        "            print(classification_report(y_true, y_pred, target_names=[\"Male\", \"Female\"]))\n",
        "            print(\"Confusion Matrix:\")\n",
        "            print(confusion_matrix(y_true, y_pred))\n",
        "\n",
        "            # Save model based on better gender accuracy\n",
        "            if val_gender_acc > best_val_gender_acc:\n",
        "                best_val_gender_acc = val_gender_acc\n",
        "                torch.save(self.net.state_dict(), save_path)\n",
        "                print(f\"ğŸ’¾ Saved best model to {save_path} (Gender Acc: {val_gender_acc:.4f})\")\n",
        "\n",
        "            scheduler.step(val_combined_loss)\n",
        "\n",
        "\n",
        "    def predict(self, image_path, min_face_size=40, min_confidence=0.5):\n",
        "        \"\"\"Predict age and gender for all faces in an image\"\"\"\n",
        "        # Load image\n",
        "        img = cv2.imread(image_path)\n",
        "        if img is None:\n",
        "            print(f\"âŒ Could not read image: {image_path}\")\n",
        "            return []\n",
        "\n",
        "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        detections = []\n",
        "\n",
        "        # Detect persons\n",
        "        person_results = self.person_detector(img_rgb, conf=0.5, classes=[0])\n",
        "        person_boxes = []\n",
        "        if hasattr(person_results[0], 'boxes') and person_results[0].boxes is not None:\n",
        "            for box in person_results[0].boxes:\n",
        "                if int(box.cls) == 0:  # Person class\n",
        "                    x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())\n",
        "                    person_boxes.append((x1, y1, x2, y2))\n",
        "\n",
        "        # Detect faces\n",
        "        face_results = self.face_detector(img_rgb, conf=0.3)\n",
        "        face_boxes = []\n",
        "        if hasattr(face_results[0], 'boxes') and face_results[0].boxes is not None:\n",
        "            for box in face_results[0].boxes:\n",
        "                if int(box.cls) == 0:  # Face class\n",
        "                    x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())\n",
        "                    conf = box.conf.item()\n",
        "                    # Skip small faces\n",
        "                    if (x2 - x1) < min_face_size or (y2 - y1) < min_face_size:\n",
        "                        continue\n",
        "                    face_boxes.append((x1, y1, x2, y2, conf))\n",
        "\n",
        "        # Process each detected person\n",
        "        for pbox in person_boxes:\n",
        "            px1, py1, px2, py2 = pbox\n",
        "            # Find faces within this person\n",
        "            person_faces = []\n",
        "            for fbox in face_boxes:\n",
        "                fx1, fy1, fx2, fy2, conf = fbox\n",
        "                # Check if face center is within person box\n",
        "                cx = (fx1 + fx2) // 2\n",
        "                cy = (fy1 + fy2) // 2\n",
        "                if (px1 <= cx <= px2) and (py1 <= cy <= py2):\n",
        "                    person_faces.append((fx1, fy1, fx2, fy2, conf))\n",
        "\n",
        "            # Process faces for this person\n",
        "            for face in person_faces:\n",
        "                fx1, fy1, fx2, fy2, conf = face\n",
        "                face_img = img_rgb[fy1:fy2, fx1:fx2]\n",
        "                if face_img.size == 0:\n",
        "                    continue\n",
        "\n",
        "                # Predict attributes\n",
        "                with torch.no_grad():\n",
        "                    face_pil = Image.fromarray(face_img)\n",
        "                    t = self.transform(face_pil).unsqueeze(0).to(DEVICE)\n",
        "                    age_pred, gender_pred = self.net(t)\n",
        "\n",
        "                    # Process predictions\n",
        "                    age = age_pred[0][0].item() * MAX_AGE\n",
        "                    gender_prob = torch.sigmoid(gender_pred[0][0]).item()\n",
        "                    gender = \"female\" if gender_prob > 0.5 else \"male\"\n",
        "                    gender_conf = gender_prob if gender == \"female\" else 1 - gender_prob\n",
        "\n",
        "                    # Combined confidence\n",
        "                    face_conf = min(conf, (conf + gender_conf) / 2)\n",
        "\n",
        "                if face_conf >= min_confidence:\n",
        "                    detections.append({\n",
        "                        \"person_bbox\": (px1, py1, px2, py2),\n",
        "                        \"face_bbox\": (fx1, fy1, fx2, fy2),\n",
        "                        \"age\": age,\n",
        "                        \"gender\": gender,\n",
        "                        \"conf\": face_conf,\n",
        "                        \"gender_conf\": gender_conf\n",
        "                    })\n",
        "\n",
        "        # Process standalone faces not in any person\n",
        "        for fbox in face_boxes:\n",
        "            fx1, fy1, fx2, fy2, conf = fbox\n",
        "            # Check if already processed\n",
        "            processed = any(fx1 == d[\"face_bbox\"][0] for d in detections)\n",
        "            if processed:\n",
        "                continue\n",
        "\n",
        "            face_img = img_rgb[fy1:fy2, fx1:fx2]\n",
        "            if face_img.size == 0:\n",
        "                continue\n",
        "\n",
        "            # Create approximate person box\n",
        "            w, h = fx2 - fx1, fy2 - fy1\n",
        "            px1 = max(0, fx1 - w//2)\n",
        "            py1 = max(0, fy1 - h//2)\n",
        "            px2 = min(img.shape[1], fx2 + w//2)\n",
        "            py2 = min(img.shape[0], fy2 + h*2)\n",
        "\n",
        "            # Predict attributes\n",
        "            with torch.no_grad():\n",
        "                face_pil = Image.fromarray(face_img)\n",
        "                t = self.transform(face_pil).unsqueeze(0).to(DEVICE)\n",
        "                age_pred, gender_pred = self.net(t)\n",
        "\n",
        "                # Process predictions\n",
        "                age = age_pred[0][0].item() * MAX_AGE\n",
        "                gender_prob = torch.sigmoid(gender_pred[0][0]).item()\n",
        "                gender = \"female\" if gender_prob > 0.5 else \"male\"\n",
        "                gender_conf = gender_prob if gender == \"female\" else 1 - gender_prob\n",
        "\n",
        "                # Combined confidence\n",
        "                face_conf = min(conf, (conf + gender_conf) / 2)\n",
        "\n",
        "            if face_conf >= min_confidence:\n",
        "                detections.append({\n",
        "                    \"person_bbox\": (px1, py1, px2, py2),\n",
        "                    \"face_bbox\": (fx1, fy1, fx2, fy2),\n",
        "                    \"age\": age,\n",
        "                    \"gender\": gender,\n",
        "                    \"conf\": face_conf,\n",
        "                    \"gender_conf\": gender_conf\n",
        "                })\n",
        "\n",
        "        return detections\n",
        "\n",
        "# â”€â”€â”€ Visualization â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def visualize_results(image_path, detections, min_confidence=0.5):\n",
        "    \"\"\"Draw detection results on the image\"\"\"\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        print(f\"âŒ Could not read image: {image_path}\")\n",
        "        return \"\"\n",
        "\n",
        "    # Define colors\n",
        "    person_color = (0, 255, 0)  # Green for person\n",
        "    face_color = (0, 0, 255)    # Red for face\n",
        "    text_color = (255, 255, 255) # White text\n",
        "\n",
        "    for d in detections:\n",
        "        if d[\"conf\"] < min_confidence:\n",
        "            continue\n",
        "\n",
        "        # Unpack detections\n",
        "        px1, py1, px2, py2 = d[\"person_bbox\"]\n",
        "        fx1, fy1, fx2, fy2 = d[\"face_bbox\"]\n",
        "        gender = d[\"gender\"]\n",
        "        age = d[\"age\"]\n",
        "        conf = d[\"conf\"]\n",
        "\n",
        "        # Draw boxes\n",
        "        cv2.rectangle(img, (px1, py1), (px2, py2), person_color, 2)\n",
        "        cv2.rectangle(img, (fx1, fy1), (fx2, fy2), face_color, 2)\n",
        "\n",
        "        # Label\n",
        "        label = f\"{gender} {age:.1f}y (Conf: {conf:.2f})\"\n",
        "        cv2.putText(img, label, (px1, py1 - 10),\n",
        "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, text_color, 2)\n",
        "\n",
        "    # Save output\n",
        "    out_path = f\"output_{os.path.basename(image_path)}\"\n",
        "    cv2.imwrite(out_path, img)\n",
        "    print(f\"ğŸ’¾ Saved visualization to {out_path}\")\n",
        "    return out_path\n",
        "\n",
        "# â”€â”€â”€ Main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "if __name__ == \"__main__\":\n",
        "    # Training\n",
        "    if TRAIN_MODEL:\n",
        "        print(\"ğŸ” Loading dataset...\")\n",
        "        try:\n",
        "            transform = transforms.Compose([\n",
        "                transforms.Resize(256),\n",
        "                transforms.CenterCrop(224),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "            ])\n",
        "            dataset = FaceAgeGenderDataset(FACE_DATASET_PATH, transform=transform, max_samples=5000)\n",
        "            print(f\"âœ… Loaded dataset with {len(dataset)} samples\")\n",
        "\n",
        "            # Initialize and train detector\n",
        "            detector = FaceAttributeDetector(backbone=\"resnet50\")\n",
        "            detector.train(dataset, epochs=20, batch_size=64)\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error during training: {str(e)}\")\n",
        "\n",
        "    # Inference\n",
        "    if RUN_INFERENCE:\n",
        "        # Initialize detector with trained model\n",
        "        detector = FaceAttributeDetector(model_path=MODEL_PATH, backbone=\"resnet50\")\n",
        "\n",
        "        for img_path in TEST_IMAGES:\n",
        "            if not os.path.exists(img_path):\n",
        "                print(f\"âŒ Image not found: {img_path}\")\n",
        "                continue\n",
        "\n",
        "            print(f\"\\nğŸ” Processing {os.path.basename(img_path)}...\")\n",
        "            try:\n",
        "                start_time = time.time()\n",
        "                detections = detector.predict(img_path, min_confidence=0.4)\n",
        "                proc_time = time.time() - start_time\n",
        "\n",
        "                if detections:\n",
        "                    print(f\"âœ… Found {len(detections)} detections in {proc_time:.2f}s\")\n",
        "                    for i, d in enumerate(detections, 1):\n",
        "                        print(f\"{i}. {d['gender']} (Age: {d['age']:.1f}, Conf: {d['conf']:.2f})\")\n",
        "                    visualize_results(img_path, detections)\n",
        "                else:\n",
        "                    print(\"âŒ No valid detections found\")\n",
        "            except Exception as e:\n",
        "                print(f\"âŒ Error during inference: {str(e)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torchvision import transforms, models\n",
        "from ultralytics import YOLO\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import glob\n",
        "import re\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Configuration\n",
        "TRAIN_MODEL = True\n",
        "RUN_INFERENCE = True\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Paths\n",
        "FACE_DATASET_PATH = \"/content/drive/MyDrive/crop_part1\"\n",
        "MODEL_PATH = \"face_age_gender_model.pth\"\n",
        "TEST_IMAGES = [\n",
        "    \"/content/Two girls.jpg\",\n",
        "    \"/content/download.jpg\",\n",
        "    \"/content/how-to-be-a-people-person-1662995088.jpg\"\n",
        "]\n",
        "MAX_AGE = 100.0\n",
        "\n",
        "# Download YOLO face model if needed\n",
        "if not os.path.exists(\"yolov8n-face.pt\"):\n",
        "    torch.hub.download_url_to_file(\"https://github.com/ultralytics/assets/releases/download/v8.1.0/yolov8n-face.pt\", \"yolov8n-face.pt\")\n",
        "\n",
        "# â”€â”€â”€ Dataset â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "class FaceAgeGenderDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None, max_samples=None):\n",
        "        self.transform = transform\n",
        "        paths = glob.glob(os.path.join(root_dir, \"**\", \"*.jpg\"), recursive=True)\n",
        "        paths += glob.glob(os.path.join(root_dir, \"**\", \"*.jpeg\"), recursive=True)\n",
        "        paths += glob.glob(os.path.join(root_dir, \"**\", \"*.png\"), recursive=True)\n",
        "        self.samples = []\n",
        "\n",
        "        for p in paths:\n",
        "            fn = os.path.basename(p)\n",
        "            m = re.match(r\"(\\d+)_([01])_.*\\.(?:jpg|jpeg|png)\", fn)\n",
        "            if m:\n",
        "                try:\n",
        "                    age = min(int(m.group(1)), MAX_AGE)\n",
        "                    gender = int(m.group(2))\n",
        "                    self.samples.append((p, age, gender))\n",
        "                    if max_samples and len(self.samples) >= max_samples:\n",
        "                        break\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "        if not self.samples:\n",
        "            raise RuntimeError(\"No valid samples found\")\n",
        "        print(f\"Loaded {len(self.samples)} face images\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        p, age, gender = self.samples[idx]\n",
        "        try:\n",
        "            img = Image.open(p).convert(\"RGB\")\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "            norm_age = age / MAX_AGE\n",
        "            return img, norm_age, gender\n",
        "        except:\n",
        "            img = Image.new(\"RGB\", (224, 224), (0, 0, 0))\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "            return img, 0.0, 0\n",
        "\n",
        "# â”€â”€â”€ Model with Attention â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "class AgeGenderModel(nn.Module):\n",
        "    def __init__(self, backbone=\"resnet50\"):\n",
        "        super().__init__()\n",
        "        if backbone == \"resnet34\":\n",
        "            base = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)\n",
        "            feat_dim = 512\n",
        "        else:\n",
        "            base = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
        "            feat_dim = 2048\n",
        "\n",
        "        self.features = nn.Sequential(*list(base.children())[:-1])\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Conv2d(feat_dim, feat_dim//8, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(feat_dim//8, 1, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.age_head = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(feat_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.gender_head = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(feat_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.features(x)\n",
        "        att = self.attention(features)\n",
        "        weighted_features = features * att\n",
        "        pooled = F.adaptive_avg_pool2d(weighted_features, (1, 1))\n",
        "        age = self.age_head(pooled)\n",
        "        gender = self.gender_head(pooled)\n",
        "        return age, gender\n",
        "\n",
        "# â”€â”€â”€ Face Attribute Detector â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "class FaceAttributeDetector:\n",
        "    def __init__(self, model_path=None, backbone=\"resnet50\"):\n",
        "        self.face_detector = YOLO(\"yolov8n-face.pt\")\n",
        "        self.net = AgeGenderModel(backbone).to(DEVICE)\n",
        "        self.backbone = backbone\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "        if model_path and os.path.exists(model_path):\n",
        "            try:\n",
        "                self.net.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
        "                print(\"âœ… Loaded pretrained age/gender model\")\n",
        "            except Exception as e:\n",
        "                print(f\"âš ï¸ Failed to load model weights: {e}\")\n",
        "\n",
        "    def train(self, dataset, epochs=20, batch_size=64, save_path=MODEL_PATH):\n",
        "        train_size = int(0.9 * len(dataset))\n",
        "        val_size = len(dataset) - train_size\n",
        "        train_set = FaceAgeGenderDataset(\n",
        "            FACE_DATASET_PATH,\n",
        "            transform=transforms.Compose([\n",
        "                transforms.Resize(256),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.RandomRotation(10),\n",
        "                transforms.RandomAffine(0, shear=10),\n",
        "                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "                transforms.RandomCrop(224),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "                transforms.RandomErasing(p=0.3, scale=(0.02, 0.2))\n",
        "            ]),\n",
        "            max_samples=train_size\n",
        "        )\n",
        "        val_set = FaceAgeGenderDataset(\n",
        "            FACE_DATASET_PATH,\n",
        "            transform=transforms.Compose([\n",
        "                transforms.Resize(256),\n",
        "                transforms.CenterCrop(224),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "            ]),\n",
        "            max_samples=val_size\n",
        "        )\n",
        "        train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=min(4, os.cpu_count()))\n",
        "        val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=min(2, os.cpu_count()))\n",
        "\n",
        "        age_loss_fn = nn.MSELoss()\n",
        "        gender_loss_fn = nn.BCEWithLogitsLoss()\n",
        "        optimizer = optim.AdamW(self.net.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
        "\n",
        "        print(f\"ğŸš€ Training on {len(train_set)} samples, validating on {len(val_set)} samples\")\n",
        "        best_val_gender_acc = 0.0\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            self.net.train()\n",
        "            train_age_loss, train_gender_loss = 0.0, 0.0\n",
        "            correct_gender, total_samples = 0, 0\n",
        "\n",
        "            for imgs, ages, genders in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
        "                imgs = imgs.to(DEVICE)\n",
        "                ages = ages.float().to(DEVICE)\n",
        "                genders = genders.float().to(DEVICE).unsqueeze(1)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                age_preds, gender_preds = self.net(imgs)\n",
        "\n",
        "                age_loss = age_loss_fn(age_preds.squeeze(), ages)\n",
        "                gender_loss = gender_loss_fn(gender_preds, genders)\n",
        "                loss = 0.5 * age_loss + 0.5 * gender_loss\n",
        "\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.net.parameters(), 1.0)\n",
        "                optimizer.step()\n",
        "\n",
        "                train_age_loss += age_loss.item()\n",
        "                train_gender_loss += gender_loss.item()\n",
        "                gender_preds_bin = (torch.sigmoid(gender_preds) > 0.5).float()\n",
        "                correct_gender += (gender_preds_bin == genders).sum().item()\n",
        "                total_samples += imgs.size(0)\n",
        "\n",
        "            self.net.eval()\n",
        "            val_age_loss, val_gender_loss = 0.0, 0.0\n",
        "            val_correct_gender, val_total = 0, 0\n",
        "            y_true, y_pred = [], []\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for imgs, ages, genders in val_loader:\n",
        "                    imgs = imgs.to(DEVICE)\n",
        "                    ages = ages.float().to(DEVICE)\n",
        "                    genders = genders.float().to(DEVICE).unsqueeze(1)\n",
        "\n",
        "                    age_preds, gender_preds = self.net(imgs)\n",
        "                    val_age_loss += age_loss_fn(age_preds.squeeze(), ages).item()\n",
        "                    val_gender_loss += gender_loss_fn(gender_preds, genders).item()\n",
        "                    gender_preds_bin = (torch.sigmoid(gender_preds) > 0.5).float()\n",
        "                    val_correct_gender += (gender_preds_bin == genders).sum().item()\n",
        "                    val_total += imgs.size(0)\n",
        "                    y_true.extend(genders.cpu().numpy().flatten())\n",
        "                    y_pred.extend(gender_preds_bin.cpu().numpy().flatten())\n",
        "\n",
        "            train_age_loss /= len(train_loader)\n",
        "            train_gender_loss /= len(train_loader)\n",
        "            val_age_loss /= len(val_loader)\n",
        "            val_gender_loss /= len(val_loader)\n",
        "            train_gender_acc = correct_gender / total_samples\n",
        "            val_gender_acc = val_correct_gender / val_total\n",
        "            val_combined_loss = 0.5 * val_age_loss + 0.5 * val_gender_loss\n",
        "\n",
        "            print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "            print(f\"Train - Age Loss: {train_age_loss:.4f} | Gender Loss: {train_gender_loss:.4f} | Gender Acc: {train_gender_acc:.4f}\")\n",
        "            print(f\"Val   - Age Loss: {val_age_loss:.4f} | Gender Loss: {val_gender_loss:.4f} | Gender Acc: {val_gender_acc:.4f}\")\n",
        "            print(\"Classification Report:\")\n",
        "            print(classification_report(y_true, y_pred, target_names=[\"Male\", \"Female\"]))\n",
        "            print(\"Confusion Matrix:\")\n",
        "            print(confusion_matrix(y_true, y_pred))\n",
        "\n",
        "            if val_gender_acc > best_val_gender_acc:\n",
        "                best_val_gender_acc = val_gender_acc\n",
        "                torch.save(self.net.state_dict(), save_path)\n",
        "                print(f\"ğŸ’¾ Saved best model to {save_path} (Gender Acc: {val_gender_acc:.4f})\")\n",
        "\n",
        "            scheduler.step(val_combined_loss)\n",
        "\n",
        "    def predict(self, image_path, min_face_size=30, min_confidence=0.3):\n",
        "        img = cv2.imread(image_path)\n",
        "        if img is None:\n",
        "            print(f\"âŒ Could not read image: {image_path}\")\n",
        "            return []\n",
        "\n",
        "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        detections = []\n",
        "\n",
        "        # Face detection with lower confidence threshold\n",
        "        face_results = self.face_detector(img_rgb, conf=0.2)\n",
        "        face_boxes = []\n",
        "        if hasattr(face_results[0], 'boxes') and face_results[0].boxes is not None:\n",
        "            for box in face_results[0].boxes:\n",
        "                if int(box.cls) == 0:\n",
        "                    x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())\n",
        "                    conf = box.conf.item()\n",
        "                    if (x2 - x1) < min_face_size or (y2 - y1) < min_face_size:\n",
        "                        continue\n",
        "                    face_boxes.append((x1, y1, x2, y2, conf))\n",
        "\n",
        "        for fbox in face_boxes:\n",
        "            fx1, fy1, fx2, fy2, conf = fbox\n",
        "            face_img = img_rgb[fy1:fy2, fx1:fx2]\n",
        "            if face_img.size == 0:\n",
        "                continue\n",
        "\n",
        "            with torch.no_grad():\n",
        "                face_pil = Image.fromarray(face_img)\n",
        "                t = self.transform(face_pil).unsqueeze(0).to(DEVICE)\n",
        "                age_pred, gender_pred = self.net(t)\n",
        "\n",
        "                age = age_pred[0][0].item() * MAX_AGE\n",
        "                gender_prob = torch.sigmoid(gender_pred[0][0]).item()\n",
        "                gender = \"female\" if gender_prob > 0.5 else \"male\"\n",
        "                gender_conf = gender_prob if gender == \"female\" else 1 - gender_prob\n",
        "                face_conf = (conf + gender_conf) / 2\n",
        "\n",
        "            if face_conf >= min_confidence:\n",
        "                detections.append({\n",
        "                    \"face_bbox\": (fx1, fy1, fx2, fy2),\n",
        "                    \"age\": age,\n",
        "                    \"gender\": gender,\n",
        "                    \"conf\": face_conf,\n",
        "                    \"gender_conf\": gender_conf\n",
        "                })\n",
        "\n",
        "        return detections\n",
        "\n",
        "# â”€â”€â”€ Visualization â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def visualize_results(image_path, detections, min_confidence=0.3):\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        print(f\"âŒ Could not read image: {image_path}\")\n",
        "        return \"\"\n",
        "\n",
        "    face_color = (0, 0, 255)\n",
        "    text_color = (255, 255, 255)\n",
        "\n",
        "    for d in detections:\n",
        "        if d[\"conf\"] < min_confidence:\n",
        "            continue\n",
        "        fx1, fy1, fx2, fy2 = d[\"face_bbox\"]\n",
        "        gender = d[\"gender\"]\n",
        "        age = d[\"age\"]\n",
        "        conf = d[\"conf\"]\n",
        "        cv2.rectangle(img, (fx1, fy1), (fx2, fy2), face_color, 2)\n",
        "        label = f\"{gender} {age:.1f}y (Conf: {conf:.2f})\"\n",
        "        cv2.putText(img, label, (fx1, fy1 - 10),\n",
        "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, text_color, 2)\n",
        "\n",
        "    out_path = f\"output_{os.path.basename(image_path)}\"\n",
        "    cv2.imwrite(out_path, img)\n",
        "    print(f\"ğŸ’¾ Saved visualization to {out_path}\")\n",
        "    return out_path\n",
        "\n",
        "# â”€â”€â”€ Main â”€â”€â”€â”€\n",
        "if __name__ == \"__main__\":\n",
        "    if TRAIN_MODEL:\n",
        "        print(\"ğŸ” Loading dataset...\")\n",
        "        try:\n",
        "            transform = transforms.Compose([\n",
        "                transforms.Resize(256),\n",
        "                transforms.CenterCrop(224),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "            ])\n",
        "            dataset = FaceAgeGenderDataset(FACE_DATASET_PATH, transform=transform, max_samples=5000)\n",
        "            print(f\"âœ… Loaded dataset with {len(dataset)} samples\")\n",
        "            detector = FaceAttributeDetector(backbone=\"resnet50\")\n",
        "            detector.train(dataset, epochs=20, batch_size=64)\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error during training: {str(e)}\")\n",
        "\n",
        "    if RUN_INFERENCE:\n",
        "        detector = FaceAttributeDetector(model_path=MODEL_PATH, backbone=\"resnet50\")\n",
        "        for img_path in TEST_IMAGES:\n",
        "            if not os.path.exists(img_path):\n",
        "                print(f\"âŒ Image not found: {img_path}\")\n",
        "                continue\n",
        "            print(f\"\\nğŸ” Processing {os.path.basename(img_path)}...\")\n",
        "            try:\n",
        "                start_time = time.time()\n",
        "                detections = detector.predict(img_path, min_confidence=0.3)\n",
        "                proc_time = time.time() - start_time\n",
        "                if detections:\n",
        "                    print(f\"âœ… Found {len(detections)} detections in {proc_time:.2f}s\")\n",
        "                    for i, d in enumerate(detections, 1):\n",
        "                        print(f\"{i}. {d['gender']} (Age: {d['age']:.1f}, Conf: {d['conf']:.2f})\")\n",
        "                    visualize_results(img_path, detections)\n",
        "                else:\n",
        "                    print(\"âŒ No valid detections found\")\n",
        "            except Exception as e:\n",
        "                print(f\"âŒ Error during inference: {str(e)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30dPc6LTUCda",
        "outputId": "dff66848-59d8-4c26-bd12-959c81c8c6e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ” Loading dataset...\n",
            "Loaded 5000 face images\n",
            "âœ… Loaded dataset with 5000 samples\n",
            "Loaded 4500 face images\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 500 face images\n",
            "ğŸš€ Training on 4500 samples, validating on 500 samples\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [01:40<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/20\n",
            "Train - Age Loss: 0.0289 | Gender Loss: 0.6465 | Gender Acc: 0.6089\n",
            "Val   - Age Loss: 0.0194 | Gender Loss: 0.5477 | Gender Acc: 0.7040\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.89      0.39      0.55       226\n",
            "      Female       0.66      0.96      0.78       274\n",
            "\n",
            "    accuracy                           0.70       500\n",
            "   macro avg       0.77      0.68      0.66       500\n",
            "weighted avg       0.76      0.70      0.67       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[ 89 137]\n",
            " [ 11 263]]\n",
            "ğŸ’¾ Saved best model to face_age_gender_model.pth (Gender Acc: 0.7040)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:49<00:00,  1.43it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 2/20\n",
            "Train - Age Loss: 0.0200 | Gender Loss: 0.4765 | Gender Acc: 0.7709\n",
            "Val   - Age Loss: 0.0115 | Gender Loss: 0.3239 | Gender Acc: 0.8460\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.89      0.75      0.82       226\n",
            "      Female       0.82      0.92      0.87       274\n",
            "\n",
            "    accuracy                           0.85       500\n",
            "   macro avg       0.85      0.84      0.84       500\n",
            "weighted avg       0.85      0.85      0.84       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[170  56]\n",
            " [ 21 253]]\n",
            "ğŸ’¾ Saved best model to face_age_gender_model.pth (Gender Acc: 0.8460)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:50<00:00,  1.40it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 3/20\n",
            "Train - Age Loss: 0.0160 | Gender Loss: 0.3614 | Gender Acc: 0.8289\n",
            "Val   - Age Loss: 0.0092 | Gender Loss: 0.3168 | Gender Acc: 0.8640\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.94      0.74      0.83       226\n",
            "      Female       0.82      0.96      0.89       274\n",
            "\n",
            "    accuracy                           0.86       500\n",
            "   macro avg       0.88      0.85      0.86       500\n",
            "weighted avg       0.88      0.86      0.86       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[168  58]\n",
            " [ 10 264]]\n",
            "ğŸ’¾ Saved best model to face_age_gender_model.pth (Gender Acc: 0.8640)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:49<00:00,  1.42it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 4/20\n",
            "Train - Age Loss: 0.0137 | Gender Loss: 0.3139 | Gender Acc: 0.8549\n",
            "Val   - Age Loss: 0.0087 | Gender Loss: 0.2553 | Gender Acc: 0.8780\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.89      0.83      0.86       226\n",
            "      Female       0.87      0.92      0.89       274\n",
            "\n",
            "    accuracy                           0.88       500\n",
            "   macro avg       0.88      0.87      0.88       500\n",
            "weighted avg       0.88      0.88      0.88       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[187  39]\n",
            " [ 22 252]]\n",
            "ğŸ’¾ Saved best model to face_age_gender_model.pth (Gender Acc: 0.8780)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:48<00:00,  1.46it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 5/20\n",
            "Train - Age Loss: 0.0127 | Gender Loss: 0.2829 | Gender Acc: 0.8769\n",
            "Val   - Age Loss: 0.0089 | Gender Loss: 0.2240 | Gender Acc: 0.8980\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.89      0.88      0.89       226\n",
            "      Female       0.90      0.91      0.91       274\n",
            "\n",
            "    accuracy                           0.90       500\n",
            "   macro avg       0.90      0.90      0.90       500\n",
            "weighted avg       0.90      0.90      0.90       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[199  27]\n",
            " [ 24 250]]\n",
            "ğŸ’¾ Saved best model to face_age_gender_model.pth (Gender Acc: 0.8980)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:48<00:00,  1.47it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 6/20\n",
            "Train - Age Loss: 0.0115 | Gender Loss: 0.2483 | Gender Acc: 0.8900\n",
            "Val   - Age Loss: 0.0080 | Gender Loss: 0.1956 | Gender Acc: 0.9160\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.89      0.92      0.91       226\n",
            "      Female       0.94      0.91      0.92       274\n",
            "\n",
            "    accuracy                           0.92       500\n",
            "   macro avg       0.91      0.92      0.92       500\n",
            "weighted avg       0.92      0.92      0.92       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[209  17]\n",
            " [ 25 249]]\n",
            "ğŸ’¾ Saved best model to face_age_gender_model.pth (Gender Acc: 0.9160)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:49<00:00,  1.44it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 7/20\n",
            "Train - Age Loss: 0.0108 | Gender Loss: 0.2275 | Gender Acc: 0.9029\n",
            "Val   - Age Loss: 0.0084 | Gender Loss: 0.1877 | Gender Acc: 0.9060\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.96      0.83      0.89       226\n",
            "      Female       0.87      0.97      0.92       274\n",
            "\n",
            "    accuracy                           0.91       500\n",
            "   macro avg       0.92      0.90      0.90       500\n",
            "weighted avg       0.91      0.91      0.91       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[187  39]\n",
            " [  8 266]]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:47<00:00,  1.48it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 8/20\n",
            "Train - Age Loss: 0.0100 | Gender Loss: 0.1990 | Gender Acc: 0.9104\n",
            "Val   - Age Loss: 0.0073 | Gender Loss: 0.1555 | Gender Acc: 0.9420\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.94      0.93      0.94       226\n",
            "      Female       0.95      0.95      0.95       274\n",
            "\n",
            "    accuracy                           0.94       500\n",
            "   macro avg       0.94      0.94      0.94       500\n",
            "weighted avg       0.94      0.94      0.94       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[211  15]\n",
            " [ 14 260]]\n",
            "ğŸ’¾ Saved best model to face_age_gender_model.pth (Gender Acc: 0.9420)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:49<00:00,  1.44it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 9/20\n",
            "Train - Age Loss: 0.0096 | Gender Loss: 0.1877 | Gender Acc: 0.9220\n",
            "Val   - Age Loss: 0.0073 | Gender Loss: 0.1356 | Gender Acc: 0.9320\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.91      0.94      0.93       226\n",
            "      Female       0.95      0.93      0.94       274\n",
            "\n",
            "    accuracy                           0.93       500\n",
            "   macro avg       0.93      0.93      0.93       500\n",
            "weighted avg       0.93      0.93      0.93       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[212  14]\n",
            " [ 20 254]]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:49<00:00,  1.44it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 10/20\n",
            "Train - Age Loss: 0.0094 | Gender Loss: 0.1662 | Gender Acc: 0.9333\n",
            "Val   - Age Loss: 0.0070 | Gender Loss: 0.1591 | Gender Acc: 0.9400\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.90      0.98      0.94       226\n",
            "      Female       0.98      0.91      0.94       274\n",
            "\n",
            "    accuracy                           0.94       500\n",
            "   macro avg       0.94      0.94      0.94       500\n",
            "weighted avg       0.94      0.94      0.94       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[221   5]\n",
            " [ 25 249]]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:48<00:00,  1.46it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 11/20\n",
            "Train - Age Loss: 0.0092 | Gender Loss: 0.1501 | Gender Acc: 0.9396\n",
            "Val   - Age Loss: 0.0068 | Gender Loss: 0.1094 | Gender Acc: 0.9520\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.93      0.97      0.95       226\n",
            "      Female       0.97      0.94      0.96       274\n",
            "\n",
            "    accuracy                           0.95       500\n",
            "   macro avg       0.95      0.95      0.95       500\n",
            "weighted avg       0.95      0.95      0.95       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[219   7]\n",
            " [ 17 257]]\n",
            "ğŸ’¾ Saved best model to face_age_gender_model.pth (Gender Acc: 0.9520)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:48<00:00,  1.46it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 12/20\n",
            "Train - Age Loss: 0.0088 | Gender Loss: 0.1423 | Gender Acc: 0.9369\n",
            "Val   - Age Loss: 0.0066 | Gender Loss: 0.0940 | Gender Acc: 0.9620\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.94      0.98      0.96       226\n",
            "      Female       0.98      0.95      0.96       274\n",
            "\n",
            "    accuracy                           0.96       500\n",
            "   macro avg       0.96      0.96      0.96       500\n",
            "weighted avg       0.96      0.96      0.96       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[222   4]\n",
            " [ 15 259]]\n",
            "ğŸ’¾ Saved best model to face_age_gender_model.pth (Gender Acc: 0.9620)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:48<00:00,  1.46it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 13/20\n",
            "Train - Age Loss: 0.0085 | Gender Loss: 0.1359 | Gender Acc: 0.9471\n",
            "Val   - Age Loss: 0.0063 | Gender Loss: 0.0687 | Gender Acc: 0.9740\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.98      0.96      0.97       226\n",
            "      Female       0.97      0.99      0.98       274\n",
            "\n",
            "    accuracy                           0.97       500\n",
            "   macro avg       0.97      0.97      0.97       500\n",
            "weighted avg       0.97      0.97      0.97       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[217   9]\n",
            " [  4 270]]\n",
            "ğŸ’¾ Saved best model to face_age_gender_model.pth (Gender Acc: 0.9740)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:48<00:00,  1.48it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 14/20\n",
            "Train - Age Loss: 0.0085 | Gender Loss: 0.1175 | Gender Acc: 0.9507\n",
            "Val   - Age Loss: 0.0068 | Gender Loss: 0.0872 | Gender Acc: 0.9560\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       1.00      0.91      0.95       226\n",
            "      Female       0.93      1.00      0.96       274\n",
            "\n",
            "    accuracy                           0.96       500\n",
            "   macro avg       0.96      0.95      0.96       500\n",
            "weighted avg       0.96      0.96      0.96       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[205  21]\n",
            " [  1 273]]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 15/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:47<00:00,  1.49it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 15/20\n",
            "Train - Age Loss: 0.0082 | Gender Loss: 0.1075 | Gender Acc: 0.9567\n",
            "Val   - Age Loss: 0.0071 | Gender Loss: 0.1057 | Gender Acc: 0.9500\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       1.00      0.89      0.94       226\n",
            "      Female       0.92      1.00      0.96       274\n",
            "\n",
            "    accuracy                           0.95       500\n",
            "   macro avg       0.96      0.95      0.95       500\n",
            "weighted avg       0.95      0.95      0.95       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[202  24]\n",
            " [  1 273]]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 16/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:47<00:00,  1.48it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 16/20\n",
            "Train - Age Loss: 0.0083 | Gender Loss: 0.0938 | Gender Acc: 0.9642\n",
            "Val   - Age Loss: 0.0062 | Gender Loss: 0.0934 | Gender Acc: 0.9620\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.98      0.93      0.96       226\n",
            "      Female       0.95      0.99      0.97       274\n",
            "\n",
            "    accuracy                           0.96       500\n",
            "   macro avg       0.96      0.96      0.96       500\n",
            "weighted avg       0.96      0.96      0.96       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[211  15]\n",
            " [  4 270]]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 17/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:47<00:00,  1.48it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 17/20\n",
            "Train - Age Loss: 0.0077 | Gender Loss: 0.0931 | Gender Acc: 0.9602\n",
            "Val   - Age Loss: 0.0058 | Gender Loss: 0.0401 | Gender Acc: 0.9840\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.99      0.97      0.98       226\n",
            "      Female       0.98      0.99      0.99       274\n",
            "\n",
            "    accuracy                           0.98       500\n",
            "   macro avg       0.98      0.98      0.98       500\n",
            "weighted avg       0.98      0.98      0.98       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[220   6]\n",
            " [  2 272]]\n",
            "ğŸ’¾ Saved best model to face_age_gender_model.pth (Gender Acc: 0.9840)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:48<00:00,  1.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 18/20\n",
            "Train - Age Loss: 0.0078 | Gender Loss: 0.0968 | Gender Acc: 0.9636\n",
            "Val   - Age Loss: 0.0073 | Gender Loss: 0.0476 | Gender Acc: 0.9860\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       1.00      0.97      0.98       226\n",
            "      Female       0.98      1.00      0.99       274\n",
            "\n",
            "    accuracy                           0.99       500\n",
            "   macro avg       0.99      0.98      0.99       500\n",
            "weighted avg       0.99      0.99      0.99       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[220   6]\n",
            " [  1 273]]\n",
            "ğŸ’¾ Saved best model to face_age_gender_model.pth (Gender Acc: 0.9860)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:48<00:00,  1.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 19/20\n",
            "Train - Age Loss: 0.0075 | Gender Loss: 0.0873 | Gender Acc: 0.9664\n",
            "Val   - Age Loss: 0.0062 | Gender Loss: 0.0274 | Gender Acc: 0.9920\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.99      0.99      0.99       226\n",
            "      Female       0.99      0.99      0.99       274\n",
            "\n",
            "    accuracy                           0.99       500\n",
            "   macro avg       0.99      0.99      0.99       500\n",
            "weighted avg       0.99      0.99      0.99       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[224   2]\n",
            " [  2 272]]\n",
            "ğŸ’¾ Saved best model to face_age_gender_model.pth (Gender Acc: 0.9920)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:48<00:00,  1.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 20/20\n",
            "Train - Age Loss: 0.0072 | Gender Loss: 0.0782 | Gender Acc: 0.9744\n",
            "Val   - Age Loss: 0.0053 | Gender Loss: 0.0266 | Gender Acc: 0.9900\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Male       0.99      0.99      0.99       226\n",
            "      Female       0.99      0.99      0.99       274\n",
            "\n",
            "    accuracy                           0.99       500\n",
            "   macro avg       0.99      0.99      0.99       500\n",
            "weighted avg       0.99      0.99      0.99       500\n",
            "\n",
            "Confusion Matrix:\n",
            "[[223   3]\n",
            " [  2 272]]\n",
            "âœ… Loaded pretrained age/gender model\n",
            "\n",
            "ğŸ” Processing Two girls.jpg...\n",
            "\n",
            "0: 480x640 2 faces, 11.7ms\n",
            "Speed: 2.3ms preprocess, 11.7ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "âœ… Found 1 detections in 0.16s\n",
            "1. female (Age: 41.3, Conf: 0.82)\n",
            "ğŸ’¾ Saved visualization to output_Two girls.jpg\n",
            "\n",
            "ğŸ” Processing download.jpg...\n",
            "\n",
            "0: 448x640 23 faces, 11.1ms\n",
            "Speed: 3.5ms preprocess, 11.1ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
            "âŒ No valid detections found\n",
            "\n",
            "ğŸ” Processing how-to-be-a-people-person-1662995088.jpg...\n",
            "\n",
            "0: 352x640 5 faces, 9.8ms\n",
            "Speed: 2.9ms preprocess, 9.8ms inference, 1.6ms postprocess per image at shape (1, 3, 352, 640)\n",
            "âœ… Found 5 detections in 0.09s\n",
            "1. female (Age: 36.2, Conf: 0.74)\n",
            "2. male (Age: 32.4, Conf: 0.79)\n",
            "3. male (Age: 34.4, Conf: 0.72)\n",
            "4. female (Age: 34.1, Conf: 0.77)\n",
            "5. female (Age: 26.2, Conf: 0.80)\n",
            "ğŸ’¾ Saved visualization to output_how-to-be-a-people-person-1662995088.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vb_n8hQXnPJ9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}